{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as Math\n",
    "import matplotlib\n",
    "matplotlib.use(\"Pdf\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def date2time(date):\n",
    "    time_array=date.split()\n",
    "    time_sub=time_array[1].split('.')\n",
    "    array=time_sub[0].split(':')\n",
    "    time=int(array[0])*3600+int(array[1])*60+int(array[2]) \n",
    "    return time\n",
    "\n",
    "def compute_time_interval(start, end):\n",
    "   \n",
    "    start_time = date2time(start)\n",
    "    end_time = date2time(end)\n",
    "    \n",
    "    # 相减得到秒数\n",
    "    seconds = end_time - start_time\n",
    "#     print(start, end, seconds)\n",
    "    \n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = 6378137\n",
    "rj = 6356725\n",
    "from math import atan, cos, asin, sqrt, pow, pi, sin\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0\n",
    "\n",
    "def azimuth(pt_a, pt_b):\n",
    "    lon_a, lat_a = pt_a\n",
    "    lon_b, lat_b = pt_b\n",
    "    rlon_a, rlat_a = rad(lon_a), rad(lat_a)\n",
    "    rlon_b, rlat_b = rad(lon_b), rad(lat_b)\n",
    "    ec=rj+(rc-rj)*(90.-lat_a)/90.\n",
    "    ed=ec*cos(rlat_a)\n",
    "\n",
    "    dx = (rlon_b - rlon_a) * ec\n",
    "    dy = (rlat_b - rlat_a) * ed\n",
    "    if dy == 0:\n",
    "        angle = 90. \n",
    "    else:\n",
    "        angle = atan(abs(dx / dy)) * 180.0 / pi\n",
    "    dlon = lon_b - lon_a\n",
    "    dlat = lat_b - lat_a\n",
    "    if dlon > 0 and dlat <= 0:\n",
    "        angle = (90. - angle) + 90\n",
    "    elif dlon <= 0 and dlat < 0:\n",
    "        angle = angle + 180 \n",
    "    elif dlon < 0 and dlat >= 0:\n",
    "        angle = (90. - angle) + 270 \n",
    "    return angle\n",
    "\n",
    "def distance(true_pt, pred_pt):\n",
    "    lat1 = float(true_pt[1])\n",
    "    lng1 = float(true_pt[0])\n",
    "    lat2 = float(pred_pt[1])\n",
    "    lng2 = float(pred_pt[0])\n",
    "    radLat1 = rad(lat1)\n",
    "    radLat2 = rad(lat2)\n",
    "    a = radLat1 - radLat2\n",
    "    b = rad(lng1) - rad(lng2)\n",
    "    s = 2 * Math.asin(Math.sqrt(Math.pow(Math.sin(a/2),2) +\n",
    "    Math.cos(radLat1)*Math.cos(radLat2)*Math.pow(Math.sin(b/2),2)))\n",
    "    s = s * 6378.137\n",
    "    s = round(s * 10000) / 10\n",
    "    return s\n",
    "\n",
    "def sq(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_new = [\n",
    "    #'Num_connected',\n",
    "    'TrajID',\n",
    "    'RNCID_1',\n",
    "    'CellID_1',\n",
    "    'EcNo_1',\n",
    "    'RSCP_1',\n",
    "    'RNCID_2',\n",
    "    'CellID_2',\n",
    "    'EcNo_2',\n",
    "    'RSCP_2',\n",
    "    'RNCID_3',\n",
    "    'CellID_3',\n",
    "    'EcNo_3',\n",
    "    'RSCP_3',\n",
    "    'RNCID_4',\n",
    "    'CellID_4',\n",
    "    'EcNo_4',\n",
    "    'RSCP_4',\n",
    "    'RNCID_5',\n",
    "    'CellID_5',\n",
    "    'EcNo_5',\n",
    "    'RSCP_5',\n",
    "    'RNCID_6',\n",
    "    'CellID_6',\n",
    "    'EcNo_6',\n",
    "    'RSCP_6',\n",
    "    #'RSSI_6',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_rf = [\n",
    "    'RNCID_1',\n",
    "    'CellID_1',\n",
    "    'EcNo_1',\n",
    "    'RSCP_1',\n",
    "    'RNCID_2',\n",
    "    'CellID_2',\n",
    "    'EcNo_2',\n",
    "    'RSCP_2',\n",
    "    'RNCID_3',\n",
    "    'CellID_3',\n",
    "    'EcNo_3',\n",
    "    'RSCP_3',\n",
    "    'RNCID_4',\n",
    "    'CellID_4',\n",
    "    'EcNo_4',\n",
    "    'RSCP_4',\n",
    "    'RNCID_5',\n",
    "    'CellID_5',\n",
    "    'EcNo_5',\n",
    "    'RSCP_5',\n",
    "    'RNCID_6',\n",
    "    'CellID_6',\n",
    "    'EcNo_6',\n",
    "    'RSCP_6',\n",
    "    'Lon','Lat','Lon2','Lat2','Lon3','Lat3','Lon4','Lat4','Lon5','Lat5','Lon6','Lat6'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_2g_engpara():\n",
    "    eng_para = pd.read_csv('Unicom/Eng_para/4G工参20160505.CSV', encoding='gbk')\n",
    "    eng_para = eng_para[['CGI', u'经度', u'纬度']]\n",
    "    eng_para = eng_para[eng_para['CGI'].notnull()]\n",
    "    enodebid, ci = [], []\n",
    "    # 把CGI字段拆成两列，比如把460-00-107797-3拆成107797和3两列\n",
    "    for i, v in eng_para['CGI'].iteritems():\n",
    "        cgi = v.split('-')\n",
    "        enodebid.append(int(cgi[2]))\n",
    "        ci.append(int(cgi[3]))\n",
    "    \n",
    "    # 加上两列enodebid, ci用来匹配数据\n",
    "    eng_para['LAC'] = enodebid\n",
    "    eng_para['CI'] = ci\n",
    "    eng_para = eng_para.drop(['CGI'], axis=1)\n",
    "    eng_para = eng_para[eng_para.LAC.notnull() & eng_para[u'经度'].notnull()]\n",
    "    eng_para = eng_para.drop_duplicates()\n",
    "    eng_para.rename(columns={u'经度': 'Lon', u'纬度': 'Lat'}, inplace=True)        \n",
    "    eng_para['BSID'] = range(len(eng_para))\n",
    "    eng_para['BSID'] = eng_para['BSID'].map(lambda x: x + 1)\n",
    "    \n",
    "    return eng_para\n",
    "\n",
    "def make_rf_dataset(data, eng_para):\n",
    "    for i in range(1, 7):\n",
    "        data = data.merge(eng_para, left_on=['RNCID_%d' % i, 'CellID_%d' % i], right_on=['LAC','CI'], how='left', suffixes=('', '%d' % i))\n",
    "        temp=data['CellID_%d'% i].tolist()\n",
    "        new=list()\n",
    "        for item in temp:\n",
    "            if math.isnan(item):\n",
    "                new.append(0)\n",
    "            elif int(item)<=0:\n",
    "                new.append(0)\n",
    "            else:\n",
    "                new.append(item)\n",
    "        data['CellID_%d' % i]=new\n",
    "    data = data.fillna(-999.)\n",
    "    #print data.columns\n",
    "    \n",
    "    feature = data[col_name_new+['MRTime','BSID','BSID2','BSID3','BSID4','BSID5','BSID6','Longitude', 'Latitude',\n",
    "                                 'Lon','Lat','Lon2','Lat2','Lon3','Lat3','Lon4','Lat4','Lon5','Lat5','Lon6','Lat6']]\n",
    "   \n",
    "    \n",
    "    subset=[u'Longitude', u'Latitude', \n",
    "       u'RNCID_1', u'CellID_1',u'EcNo_1',u'RSCP_1',\n",
    "       u'RNCID_2', u'CellID_2',u'EcNo_2',u'RSCP_2',\n",
    "       u'RNCID_3', u'CellID_3',u'EcNo_3',u'RSCP_3',\n",
    "       u'RNCID_4', u'CellID_4',u'EcNo_4',u'RSCP_4',\n",
    "       u'RNCID_5', u'CellID_5',u'EcNo_5',u'RSCP_5',\n",
    "       u'RNCID_6', u'CellID_6',u'EcNo_6',u'RSCP_6',\n",
    "       ]\n",
    "    #feature=feature.drop_duplicates(subset=subset) \n",
    "    label = feature[['Longitude', 'Latitude']]\n",
    "    feature= feature.drop(['Longitude', 'Latitude'],axis=1)\n",
    "    \n",
    "    return feature, label\n",
    "\n",
    "#eng_para = merge_2g_engpara()\n",
    "eng_para =merge_2g_engpara()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_model_label(error):\n",
    "    conf_l=list()\n",
    "   \n",
    "    for t in error:\n",
    "        if t<=50:\n",
    "            conf_l.append(1)\n",
    "        else:\n",
    "            conf_l.append(0)\n",
    "    \n",
    "    return conf_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Unicom/LTE Mr/_pci_mr_路测数据导出4G正向合并_903383343.csv', sep='\\t')\n",
    "df1 = df1.drop_duplicates()\n",
    "df1['TrajID'] = range(len(df1))\n",
    "df1['TrajID'] = df1['TrajID'].map(lambda x: x//20)\n",
    "\n",
    "df2 = pd.read_csv('Unicom/LTE Mr/_pci_mr_路测数据导出4G正向-2_903360656.csv', sep='\\t')\n",
    "df2 = df2.drop_duplicates()\n",
    "df2['TrajID'] = range(len(df2))\n",
    "df2['TrajID'] = df2['TrajID'].map(lambda x: x//20+df1['TrajID'].max()+1)\n",
    "\n",
    "df3 = pd.read_csv('Unicom/LTE Mr/_pci_mr_路测数据导出4G正向-3_903371421.csv', sep='\\t')\n",
    "df3 = df3.drop_duplicates()\n",
    "df3['TrajID'] = range(len(df3))\n",
    "df3['TrajID'] = df3['TrajID'].map(lambda x: x//20+df2['TrajID'].max()+1)\n",
    "\n",
    "df4 = pd.read_csv('Unicom/LTE Mr/_pci_mr_路测数据导出4G反向-1_903373250.csv', sep='\\t')\n",
    "df4 = df4.drop_duplicates()\n",
    "df4['TrajID'] = range(len(df4))\n",
    "df4['TrajID'] = df4['TrajID'].map(lambda x: x//20+df3['TrajID'].max()+1)\n",
    "\n",
    "data_2g = pd.concat([df1, df2, df3, df4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2g =data_2g.drop_duplicates([u'MRTime', u'IMSI', u'SRNCID', u'BestCellID', u'SRNTI', u'RAB',\n",
    "       u'Delay', u'UE_TXPower', u'LCS_BIT', u'Longitude', u'Latitude',\n",
    "       u'RNCID_1', u'CellID_1', u'EcNo_1', u'RSCP_1', u'RTT_1', u'UE_Rx_Tx_1',\n",
    "       u'RNCID_2', u'CellID_2', u'EcNo_2', u'RSCP_2', u'RTT_2', u'UE_Rx_Tx_2',\n",
    "       u'RNCID_3', u'CellID_3', u'EcNo_3', u'RSCP_3', u'RTT_3', u'UE_Rx_Tx_3',\n",
    "       u'RNCID_4', u'CellID_4', u'EcNo_4', u'RSCP_4', u'RTT_4', u'UE_Rx_Tx_4',\n",
    "       u'RNCID_5', u'CellID_5', u'EcNo_5', u'RSCP_5', u'RTT_5', u'UE_Rx_Tx_5',\n",
    "       u'RNCID_6', u'CellID_6', u'EcNo_6', u'RSCP_6', u'RTT_6', u'UE_Rx_Tx_6',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MRTime', 'IMSI', 'SRNCID', 'BestCellID', 'SRNTI', 'RAB', 'Delay',\n",
       "       'UE_TXPower', 'LCS_BIT', 'Longitude', 'Latitude', 'RNCID_1',\n",
       "       'CellID_1', 'EcNo_1', 'RSCP_1', 'RTT_1', 'UE_Rx_Tx_1', 'RNCID_2',\n",
       "       'CellID_2', 'EcNo_2', 'RSCP_2', 'RTT_2', 'UE_Rx_Tx_2', 'RNCID_3',\n",
       "       'CellID_3', 'EcNo_3', 'RSCP_3', 'RTT_3', 'UE_Rx_Tx_3', 'RNCID_4',\n",
       "       'CellID_4', 'EcNo_4', 'RSCP_4', 'RTT_4', 'UE_Rx_Tx_4', 'RNCID_5',\n",
       "       'CellID_5', 'EcNo_5', 'RSCP_5', 'RTT_5', 'UE_Rx_Tx_5', 'RNCID_6',\n",
       "       'CellID_6', 'EcNo_6', 'RSCP_6', 'RTT_6', 'UE_Rx_Tx_6', 'TrajID'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2g.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、邻接基站实验\n",
    "bs_num = []\n",
    "for idx,row in data_2g.iterrows():\n",
    "    for i in range(1,7):\n",
    "        if row['RNCID_%d'%i]==0 or math.isnan(row['RNCID_%d'%i]) or row['CellID_%d'%i]==-1 or math.isnan(row['CellID_%d'%i]):\n",
    "            bs_num.append(i-1)\n",
    "            break\n",
    "        if i == 6:\n",
    "            bs_num.append(i)        \n",
    "data_2g['bs_num'] = bs_num\n",
    "retain_num = int(sys.argv[1])\n",
    "# retain_num = 1\n",
    "for idx,row in data_2g.iterrows():\n",
    "    if row['bs_num']-1 <= retain_num:\n",
    "        continue\n",
    "    for i in range(retain_num+2, row['bs_num']+1):\n",
    "        data_2g.loc[idx, 'RNCID_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'CellID_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'EcNo_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'RSCP_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'RTT_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'UE_Rx_Tx_%d'%i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、基站密度实验\n",
    "bs = []\n",
    "for i in range(1, 7):\n",
    "    bs += data_2g[['RNCID_%d'% i, 'CellID_%d'% i]].values.tolist()\n",
    "bs = [tuple(t) for t in bs]\n",
    "temp = []\n",
    "[temp.append(i) for i in bs if not i in temp]\n",
    "bs = temp\n",
    "ratio = float(sys.argv[1])\n",
    "# ratio = 0.5\n",
    "drop_bs = random.sample(bs, int(len(bs) * ratio))\n",
    "for idx, row in data_2g.iterrows():\n",
    "    for i in range(1, 7):\n",
    "        if (row['RNCID_%d'% i], row['CellID_%d'% i]) in drop_bs:\n",
    "            data_2g.loc[idx, 'RNCID_%d'% i] = -999\n",
    "            data_2g.loc[idx, 'CellID_%d'% i] = -999\n",
    "            data_2g.loc[idx, 'EcNo_%d'% i] = -999\n",
    "            data_2g.loc[idx, 'RSCP_%d'% i] = -999   \n",
    "data_2g = data_2g.drop(data_2g[data_2g['RNCID_1']==-999].index)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、运动模式实验\n",
    "mode = sys.argv[1]\n",
    "if mode == '0':\n",
    "    df1 = pd.read_csv('Unicom/LTE Mr/_pci_mr_路测数据导出4G正向合并_903383343.csv', sep='\\t')\n",
    "    df1 = df1.drop_duplicates()\n",
    "    df1['TrajID'] = range(len(df1))\n",
    "    df1['TrajID'] = df1['TrajID'].map(lambda x: x//20)\n",
    "    df2 = pd.read_csv('Unicom/LTE Mr/_pci_mr_路测数据导出4G正向-2_903360656.csv', sep='\\t')\n",
    "    df2 = df2.drop_duplicates()\n",
    "    df2['TrajID'] = range(len(df2))\n",
    "    df2['TrajID'] = df2['TrajID'].map(lambda x: x//20+df1['TrajID'].max()+1)\n",
    "    df3 = pd.read_csv('Unicom/LTE Mr/_pci_mr_路测数据导出4G正向-3_903371421.csv', sep='\\t')\n",
    "    df3 = df3.drop_duplicates()\n",
    "    df3['TrajID'] = range(len(df3))\n",
    "    df3['TrajID'] = df3['TrajID'].map(lambda x: x//20+df2['TrajID'].max()+1)\n",
    "    df4 = pd.read_csv('Unicom/LTE Mr/_pci_mr_路测数据导出4G反向-1_903373250.csv', sep='\\t')\n",
    "    df4 = df4.drop_duplicates()\n",
    "    df4['TrajID'] = range(len(df4))\n",
    "    df4['TrajID'] = df4['TrajID'].map(lambda x: x//20+df3['TrajID'].max()+1)\n",
    "    data = pd.concat([df1, df2, df3, df4])\n",
    "else:\n",
    "    df1 = pd.read_csv('Unicom/LTE Mr/_pci_mr_步测数据导出4G-1_903368046.csv', sep='\\t')\n",
    "    df1 = df1.drop_duplicates()\n",
    "    df1['TrajID'] = range(len(df1))\n",
    "    df1['TrajID'] = df1['TrajID'].map(lambda x: x//20)\n",
    "    df2 = pd.read_csv('Unicom/LTE Mr/_pci_mr_步测数据导出4G-2_900298203.csv', sep='\\t')\n",
    "    df2 = df2.drop_duplicates()\n",
    "    df2['TrajID'] = range(len(df2))\n",
    "    df2['TrajID'] = df2['TrajID'].map(lambda x: x//20+df1['TrajID'].max()+1)\n",
    "    df3 = pd.read_csv('Unicom/LTE Mr/_pci_mr_步测数据导出4G-3_903182171.csv', sep='\\t')\n",
    "    df3 = df3.drop_duplicates()\n",
    "    df3['TrajID'] = range(len(df3))\n",
    "    df3['TrajID'] = df3['TrajID'].map(lambda x: x//20+df2['TrajID'].max()+1)\n",
    "    data = pd.concat([df1, df2, df3])\n",
    "\n",
    "data_2g = data.drop_duplicates(col_name_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, label = make_rf_dataset(data_2g, eng_para)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "tr_feature_r, te_feature_r, tr_label_, te_label_ = train_test_split(train, label, test_size=0.4,random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'TrajID', u'RNCID_1', u'CellID_1', u'EcNo_1', u'RSCP_1', u'RNCID_2',\n",
       "       u'CellID_2', u'EcNo_2', u'RSCP_2', u'RNCID_3', u'CellID_3', u'EcNo_3',\n",
       "       u'RSCP_3', u'RNCID_4', u'CellID_4', u'EcNo_4', u'RSCP_4', u'RNCID_5',\n",
       "       u'CellID_5', u'EcNo_5', u'RSCP_5', u'RNCID_6', u'CellID_6', u'EcNo_6',\n",
       "       u'RSCP_6', u'MRTime', u'BSID', u'BSID2', u'BSID3', u'BSID4', u'BSID5',\n",
       "       u'BSID6', u'Lon', u'Lat', u'Lon2', u'Lat2', u'Lon3', u'Lat3', u'Lon4',\n",
       "       u'Lat4', u'Lon5', u'Lat5', u'Lon6', u'Lat6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_feature_r.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_pre(data):\n",
    "    data=data.iloc[:,1:]\n",
    "    label=data[['Longitude','Latitude']]\n",
    "    data=data.drop(['Longitude', 'Latitude'],axis=1)\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_tr_feature, con_te_feature, con_tr_p, con_te_p = train_test_split(te_feature_r, te_label_, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con_tr_feature.to_csv(\"2g/conf_tr_jd2g.csv\")\n",
    "#con_te_feature.to_csv(\"2g/conf_te_jd2g.csv\")\n",
    "#tr_feature_r.to_csv(\"2g/total_conf_tr_jd2g.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid\n",
    "\n",
    "rg = grid.RoadGrid(np.vstack((tr_label_.values, te_label_.values)),80)\n",
    "tr_label_g = rg.transform(tr_label_.values, False)\n",
    "#rint tr_label_\n",
    "con_tr_j = rg.transform(con_tr_p.values, False)\n",
    "con_te_j = rg.transform(con_te_p.values, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "est=RandomForestClassifier( n_jobs=-1,\n",
    "    n_estimators =50,\n",
    "    max_features='sqrt'\n",
    ").fit(tr_feature_r[col_name_rf].values, tr_label_g)\n",
    "\n",
    "pred_tr=est.predict(tr_feature_r[col_name_rf].values)\n",
    "tr_pred = np.array([rg.grid_center[idx] for idx in pred_tr])\n",
    "error_tr = [distance(pt1, pt2) for pt1, pt2 in zip(tr_pred, tr_label_.values)]\n",
    "\n",
    "pred_con_tr=est.predict(con_tr_feature[col_name_rf].values)\n",
    "pred_con_te=est.predict(con_te_feature[col_name_rf].values)\n",
    "tr_con_pred = np.array([rg.grid_center[idx] for idx in pred_con_tr])\n",
    "te_con_pred = np.array([rg.grid_center[idx] for idx in pred_con_te])\n",
    "error_con_tr = [distance(pt1, pt2) for pt1, pt2 in zip(tr_con_pred, con_tr_p.values)]\n",
    "error_con_te = [distance(pt1, pt2) for pt1, pt2 in zip(te_con_pred, con_te_p.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(error_con_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(feature, pred, timestamp):\n",
    "    add_feature = []\n",
    "    timestamp_new=np.array(timestamp)\n",
    "    timest_array=[]\n",
    "    for item in timestamp_new:\n",
    "        time_array=item.split()\n",
    "        time_sub=time_array[1].split('.')\n",
    "        array=time_sub[0].split(':')\n",
    "        time=int(array[0])*3600+int(array[1])*60+int(array[2]) \n",
    "        timest_array.append(time)\n",
    "    for i in range(0, len(pred)):\n",
    "        if i == 0:\n",
    "            last_pt = pred[i]\n",
    "            last_time = timest_array[i]\n",
    "        else:\n",
    "            last_pt = pred[i-1]\n",
    "            last_time = timest_array[i-1]\n",
    "        if i == len(pred)-1:\n",
    "            next_pt = pred[i]\n",
    "            next_time = timest_array[i]\n",
    "        else:\n",
    "            next_pt = pred[i+1]\n",
    "            next_time = timest_array[i+1]\n",
    "        sub_add_feature = []\n",
    "        sub_add_feature.append(distance(last_pt, pred[i]))\n",
    "        sub_add_feature.append(distance(next_pt, pred[i]))\n",
    "        sub_add_feature.append(azimuth(last_pt, pred[i]))\n",
    "        sub_add_feature.append(azimuth(pred[i], next_pt))\n",
    "        sub_add_feature.append(timest_array[i]-last_time if timest_array[i]-last_time > 0 else 0)\n",
    "        sub_add_feature.append(next_time-timest_array[i] if next_time-timest_array[i] > 0 else 0)\n",
    "        sub_add_feature.append(sub_add_feature[0] / sub_add_feature[4] if sub_add_feature[4] > 0 else 0)\n",
    "        sub_add_feature.append(sub_add_feature[1] / sub_add_feature[5] if sub_add_feature[5] > 0 else 0)\n",
    "        sub_add_feature.append(last_pt[0])\n",
    "        sub_add_feature.append(last_pt[1])\n",
    "        sub_add_feature.append(next_pt[0])\n",
    "        sub_add_feature.append(next_pt[1])\n",
    "        sub_add_feature.append(pred[i][0])\n",
    "        sub_add_feature.append(pred[i][1])\n",
    "        #sub_add_feature.append(grid[i])\n",
    "        add_feature.append(sub_add_feature)\n",
    "    add_feature = np.asarray(add_feature)\n",
    "    feature = csc_matrix(np.hstack((feature, add_feature)))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tr = feature_engineer(tr_feature_r[col_name_rf], tr_pred, tr_feature_r['MRTime'])\n",
    "feature_con_tr = feature_engineer(con_tr_feature[col_name_rf], tr_con_pred, con_tr_feature['MRTime'])\n",
    "feature_con_te = feature_engineer(con_te_feature[col_name_rf], te_con_pred, con_te_feature['MRTime'])\n",
    "\n",
    "est1=RandomForestClassifier( n_jobs=-1,\n",
    "    n_estimators =50,\n",
    "    max_features='sqrt'\n",
    ").fit(feature_tr, tr_label_g)\n",
    "\n",
    "pred_con_tr=est1.predict(feature_con_tr)\n",
    "pred_con_te=est1.predict(feature_con_te)\n",
    "tr_pred = np.array([rg.grid_center[idx] for idx in pred_con_tr])\n",
    "te_pred = np.array([rg.grid_center[idx] for idx in pred_con_te])\n",
    "error_tr = [distance(pt1, pt2) for pt1, pt2 in zip(tr_pred, con_tr_p.values)]\n",
    "error_te = [distance(pt1, pt2) for pt1, pt2 in zip(te_pred, con_te_p.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.600000000000001, 31.432040816326523, 63.3)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_te = sorted(error_te)\n",
    "np.median(error_te), np.mean(error_te), error_te[int(len(error_te)*0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tr_feature_r['Longitude'] = tr_label_.iloc[:, 0].values\n",
    "tr_feature_r['Latitude'] = tr_label_.iloc[:, 1].values\n",
    "tr_feature_r['gid'] = tr_label_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_label_tr = conf_model_label(error_tr)\n",
    "conf_label_te = conf_model_label(error_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con_tr_feature[\"conf\"]= conf_label_tr\n",
    "con_te_feature[\"conf\"]= conf_label_te\n",
    "con_tr_feature[\"error\"]= error_tr\n",
    "con_te_feature[\"error\"]= error_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con_tr_feature['Longitude'] = con_tr_p.iloc[:,0]\n",
    "con_tr_feature['Latitude'] = con_tr_p.iloc[:,1]\n",
    "con_te_feature['Longitude'] = con_te_p.iloc[:,0]\n",
    "con_te_feature['Latitude'] = con_te_p.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con_tr_feature['p_gid'] = pred_con_tr\n",
    "con_tr_feature['gid'] = con_tr_j\n",
    "con_te_feature['p_gid'] = pred_con_te\n",
    "con_te_feature['gid'] = con_te_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss_level(dbm):\n",
    "    if dbm>-50:\n",
    "        return 1\n",
    "    elif dbm >-60:\n",
    "        return 2\n",
    "    elif dbm >-70:\n",
    "        return 3\n",
    "    elif dbm>-80:\n",
    "        return 4\n",
    "    elif dbm>-90:\n",
    "        return 5\n",
    "    elif dbm>-100:\n",
    "        return 6\n",
    "    elif dbm>-110:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 7):\n",
    "    con_tr_feature['Dbm_%d' % i] = con_tr_feature['RSCP_%d' % i] - con_tr_feature['EcNo_%d' % i]\n",
    "    con_te_feature['Dbm_%d' % i] = con_te_feature['RSCP_%d' % i] - con_te_feature['EcNo_%d' % i]\n",
    "    con_tr_feature['rss_level_%d' % i] = con_tr_feature['Dbm_%d' % i].map(lambda x: rss_level(x))  \n",
    "    con_te_feature['rss_level_%d' % i] = con_te_feature['Dbm_%d' % i].map(lambda x: rss_level(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob = con_tr_feature[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "               'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6',]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_bs = con_tr_feature[['BSID','BSID2','BSID3','BSID4','BSID5','BSID6',]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_rss = con_tr_feature[['rss_level_1','rss_level_2','rss_level_3',\n",
    "                               'rss_level_4','rss_level_5','rss_level_6',]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_te = con_te_feature[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "               'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6',]].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(list1, list2):\n",
    "    #print list1, list2\n",
    "    union_set = len(set(list1)|set(list2))#并集长度\n",
    "    intersection_set = len(set(list1)&set(list2))#交集长度\n",
    "\n",
    "    Jaccard = float(intersection_set/union_set) #Jaccar\n",
    "    return Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_emission_pro(jd_list, match, bs_list, ss_list, can_list, total_c, conf):\n",
    "    pro_list = []\n",
    "    weight_list = []\n",
    "    if match.shape[0]>0:\n",
    "        weight_list.append(math.log(1+match.shape[0])*1)\n",
    "        match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "             & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5])]\n",
    "        if match_ss.shape[0]>0:\n",
    "            pro_list.append(float(match_ss[match_ss['conf']==conf].shape[0])/float(total_c))\n",
    "        else:\n",
    "            pro_list.append(float(match[match['conf']==conf].shape[0])/float(total_c))\n",
    "        \n",
    "        \n",
    "    for can_temp, jd in zip(can_list, jd_list):\n",
    "        match_c = con_tr_feature[(con_tr_feature['BSID']==int(can_temp[0])) & (con_tr_feature['BSID2']==int(can_temp[1]))\n",
    "                      &(con_tr_feature['BSID3']==int(can_temp[2])) & (con_tr_feature['BSID4']==int(can_temp[3]))\n",
    "                      &(con_tr_feature['BSID5']==int(can_temp[4])) & (con_tr_feature['BSID6']==int(can_temp[5]))]\n",
    "        count = match_c.shape[0]\n",
    "        weight_list.append(math.log(1+count)*jd)\n",
    "        \n",
    "        match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                            & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                            & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])]\n",
    "        if match_ss_c.shape[0]>0:\n",
    "            pro_list.append(float(match_ss_c[match_ss_c['conf']==conf].shape[0])/float(total_c))\n",
    "        else:\n",
    "            pro_list.append(float(match_c[match_c['conf']==conf].shape[0])/float(total_c))\n",
    "\n",
    "\n",
    "    weight_sum = np.sum(weight_list)\n",
    "    ad_em_po = 0\n",
    "    \n",
    "    for x, y in zip(pro_list, weight_list):\n",
    "        ad_em_po += x * (y / weight_sum)\n",
    "    \n",
    "    return ad_em_po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_list = []\n",
    "i =0\n",
    "j=0\n",
    "zero_num = con_tr_feature[con_tr_feature['conf']==0].shape[0]\n",
    "for idx, row in total_ob_te.iterrows():\n",
    "    bs_list =row[['BSID','BSID2','BSID3','BSID4','BSID5','BSID6']].values\n",
    "    ss_list = row[['rss_level_1','rss_level_2','rss_level_3','rss_level_4','rss_level_5','rss_level_6',]].values\n",
    "    #print bs_list[0]\n",
    "    match = con_tr_feature[(con_tr_feature['BSID']==int(bs_list[0])) & (con_tr_feature['BSID2']==int(bs_list[1]))\n",
    "                          &(con_tr_feature['BSID3']==int(bs_list[2])) & (con_tr_feature['BSID4']==int(bs_list[3]))\n",
    "                          &(con_tr_feature['BSID5']==int(bs_list[4])) & (con_tr_feature['BSID6']==int(bs_list[5]))]\n",
    "    if match.shape[0]<3:\n",
    "        can_bs_row = []\n",
    "        can_j = []\n",
    "        jaccd_max = []\n",
    "        idx_list = []\n",
    "        for idxx, roww in total_ob_bs.iterrows():\n",
    "            can_bs_list = roww.values\n",
    "            jd = jaccard_sim(bs_list, can_bs_list)\n",
    "            jaccd_max.append(jd)\n",
    "            idx_list.append(idxx)\n",
    "            if jd>0.7:\n",
    "                can_bs_row.append(can_bs_list)\n",
    "                can_j.append(jd)\n",
    "                \n",
    "        if len(can_bs_row)==0:\n",
    "            can_idx = jaccd_max.index(np.max(jaccd_max))\n",
    "            can_temp = total_ob_bs.iloc[can_idx,:].values\n",
    "            can_bs_row.append(total_ob_bs.iloc[can_idx,:].values)\n",
    "            match_c = con_tr_feature[(con_tr_feature['BSID']==int(can_temp[0])) & (con_tr_feature['BSID2']==int(can_temp[1]))\n",
    "                          &(con_tr_feature['BSID3']==int(can_temp[2])) & (con_tr_feature['BSID4']==int(can_temp[3]))\n",
    "                          &(con_tr_feature['BSID5']==int(can_temp[4])) & (con_tr_feature['BSID6']==int(can_temp[5]))]\n",
    "            \n",
    "            match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                            & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                            & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])]\n",
    "            if match_ss_c.shape[0]>0:\n",
    "                zeros_list.append(float(match_ss_c[match_ss_c['conf']==0].shape[0])/float(zero_num))\n",
    "            else:\n",
    "                zeros_list.append(float(match_c[match_c['conf']==0].shape[0])/float(zero_num))\n",
    "        \n",
    "            j+=1 \n",
    "        else:\n",
    "            zeros_list.append(adaptive_emission_pro(can_j, match, bs_list, ss_list, can_bs_row, zero_num, 0))\n",
    "        #print i, len(can_bs_row)\n",
    "        i+=1\n",
    "    else:\n",
    "        j+=1\n",
    "        match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "             & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5])]\n",
    "        if match_ss.shape[0]>0:\n",
    "            zeros_list.append(float(match_ss[match_ss['conf']==0].shape[0])/float(zero_num))\n",
    "        else:\n",
    "            zeros_list.append(float(match[match['conf']==0].shape[0])/float(zero_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_te['conf_ad_em_pro_0'] = zeros_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_list = []\n",
    "one_num = con_tr_feature[con_tr_feature['conf']==1].shape[0]\n",
    "for idx, row in total_ob_te.iterrows():\n",
    "    bs_list =row[['BSID','BSID2','BSID3','BSID4','BSID5','BSID6']].values\n",
    "    ss_list = row[['rss_level_1','rss_level_2','rss_level_3','rss_level_4','rss_level_5','rss_level_6',]].values\n",
    "    #print bs_list[0]\n",
    "    match = con_tr_feature[(con_tr_feature['BSID']==int(bs_list[0])) & (con_tr_feature['BSID2']==int(bs_list[1]))\n",
    "                          &(con_tr_feature['BSID3']==int(bs_list[2])) & (con_tr_feature['BSID4']==int(bs_list[3]))\n",
    "                          &(con_tr_feature['BSID5']==int(bs_list[4])) & (con_tr_feature['BSID6']==int(bs_list[5]))]\n",
    "    if match.shape[0]<3:\n",
    "        can_bs_row = []\n",
    "        can_j = []\n",
    "        jaccd_max = []\n",
    "        idx_list = []\n",
    "        for idxx, roww in total_ob_bs.iterrows():\n",
    "            can_bs_list = roww.values\n",
    "            jd = jaccard_sim(bs_list, can_bs_list)\n",
    "            jaccd_max.append(jd)\n",
    "            idx_list.append(idxx)\n",
    "            if jd>0.7:\n",
    "                can_bs_row.append(can_bs_list)\n",
    "                can_j.append(jd)\n",
    "                \n",
    "        if len(can_bs_row)==0:\n",
    "            can_idx = jaccd_max.index(np.max(jaccd_max))\n",
    "            can_temp = total_ob_bs.iloc[can_idx,:].values\n",
    "            can_bs_row.append(total_ob_bs.iloc[can_idx,:].values)\n",
    "            match_c = con_tr_feature[(con_tr_feature['BSID']==int(can_temp[0])) & (con_tr_feature['BSID2']==int(can_temp[1]))\n",
    "                          &(con_tr_feature['BSID3']==int(can_temp[2])) & (con_tr_feature['BSID4']==int(can_temp[3]))\n",
    "                          &(con_tr_feature['BSID5']==int(can_temp[4])) & (con_tr_feature['BSID6']==int(can_temp[5]))]\n",
    "            \n",
    "            match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                            & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                            & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])]\n",
    "            if match_ss_c.shape[0]>0:\n",
    "                one_list.append(float(match_ss_c[match_ss_c['conf']==1].shape[0])/float(one_num))\n",
    "            else:\n",
    "                one_list.append(float(match_c[match_c['conf']==1].shape[0])/float(one_num))\n",
    "        \n",
    "        else:\n",
    "            one_list.append(adaptive_emission_pro(can_j, match, bs_list, ss_list, can_bs_row, one_num, 1))\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "             & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5])]\n",
    "        if match_ss.shape[0]>0:\n",
    "            one_list.append(float(match_ss[match_ss['conf']==1].shape[0])/float(one_num))\n",
    "        else:\n",
    "            one_list.append(float(match[match['conf']==1].shape[0])/float(one_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_te['conf_ad_em_pro_1'] = one_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con_te_feature['gid'] = con_te_j\n",
    "con_te_feature['p_gid'] = pred_con_te\n",
    "con_tr_feature['gid'] = con_tr_j\n",
    "con_tr_feature['p_gid'] = pred_con_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_tr = con_tr_feature.groupby(['TrajID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = []\n",
    "st_mat = np.zeros((14,2,2)) #0-0 0-1 1-0, 1-1\n",
    "for trajid, traj in trajs_tr:\n",
    "    traj = traj.sort_values(by=['MRTime'],ascending=True)\n",
    "    #print(traj['MRTime'])\n",
    "    t_time = traj['MRTime'].values\n",
    "    conf = traj['conf'].values\n",
    "    for i in range(traj.shape[0]-1):\n",
    "        time_list.append(compute_time_interval(t_time[i], t_time[i + 1]))\n",
    "        idx = int(compute_time_interval(t_time[i], t_time[i + 1])/5)\n",
    "        if idx >12:\n",
    "            idx = 13\n",
    "        if conf[i]==0 and conf[i+1]==0:\n",
    "            st_mat[idx, 0, 0] +=1\n",
    "        if conf[i] ==0 and conf[i+1] ==1:\n",
    "            st_mat[idx, 0, 1] +=1\n",
    "        if conf[i]==1 and conf[i+1]==0:\n",
    "            st_mat[idx, 1, 0] +=1\n",
    "        if conf[i] ==1 and conf[i+1] ==1:\n",
    "            st_mat[idx, 1, 1] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_te = con_te_feature.groupby(['TrajID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_t_idx = []\n",
    "#st_mat = np.zeros((14,2,2)) #0-0 0-1 1-0, 1-1\n",
    "pred_list = []\n",
    "p_g_list = []\n",
    "t_g_list = []\n",
    "t_gr_list = []\n",
    "for trajid, traj in trajs_te:\n",
    "    traj = traj.sort_values(by=['MRTime'],ascending=True)\n",
    "    t_time = traj['MRTime'].values\n",
    "    conf = traj['conf'].values\n",
    "    idx_list = []\n",
    "    for i in range(traj.shape[0]-1):\n",
    "        time_list.append(compute_time_interval(t_time[i], t_time[i + 1]))\n",
    "        idx = int(compute_time_interval(t_time[i], t_time[i + 1])/5)\n",
    "        if idx >12:\n",
    "            idx = 13\n",
    "        idx_list.append(idx)\n",
    "    pred_list.append(traj[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "               'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6',]].values)\n",
    "        \n",
    "    pred_list_t_idx.append(idx_list)\n",
    "    p_g_list.append(traj['p_gid'].values)\n",
    "    t_g_list.append(traj[['Longitude','Latitude']].values)\n",
    "    t_gr_list.append(traj['gid'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prob = [float(con_tr_feature[con_tr_feature['conf']==0].shape[0])/con_tr_feature.shape[0],\n",
    "             float(con_tr_feature[con_tr_feature['conf']==1].shape[0])/con_tr_feature.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_viterbi(st_mat, init_prob, emit_prob, obs_seq, t_list):\n",
    "    Nstate = 2\n",
    "    Nobs = int(emit_prob.shape[0])\n",
    "    T = len(obs_seq)\n",
    "    \n",
    "    partial_prob = np.zeros((Nstate,T))\n",
    "\n",
    "    path = np.zeros((Nstate,T))\n",
    "\n",
    "    for i in range(Nstate):\n",
    "        partial_prob[i,0] = init_prob[i] * emit_prob[obs_seq[0], i]\n",
    "        path[i,0] = i\n",
    "\n",
    "\n",
    "    for t in range(1,T,1):\n",
    "        newpath = np.zeros((Nstate,T))\n",
    "        for i in range(Nstate):\n",
    "            prob = -1.0\n",
    "            for j in range(Nstate):\n",
    "                nprob = partial_prob[j,t-1] * st_mat[t_list[t-1], j, i] * emit_prob[obs_seq[t], i]\n",
    "                if nprob > prob:\n",
    "                    prob = nprob\n",
    "                    partial_prob[i,t] = nprob\n",
    "                    newpath[i,0:t] = path[j,0:t]\n",
    "                    newpath[i,t] = i\n",
    "        path = newpath\n",
    "    \n",
    "    prob = -1.0\n",
    "    j = 0\n",
    "    for i in range(Nstate):\n",
    "        if(partial_prob[i,T-1] > prob):\n",
    "            prob = partial_prob[i,T-1]\n",
    "            j = i\n",
    "\n",
    "    return path[j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_pro = total_ob_te\n",
    "em_pro = em_pro.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_prob = em_pro[['conf_ad_em_pro_0', 'conf_ad_em_pro_1']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_g = rg.n_grid\n",
    "w = np.zeros((num_g, num_g+1))\n",
    "\n",
    "test = zip(pred_con_tr, con_tr_j, error_tr)\n",
    "tj_r = {}\n",
    "for ss in test:\n",
    "    #print ss\n",
    "    #w[ss[0]][num_g]=w[ss[0]][num_g]+1\n",
    "    #w[ss[0]][ss[1]]=w[ss[0]][ss[1]]+1\n",
    "    if ss[0]!=ss[1] and ss[2]>50:\n",
    "         w[ss[0]][ss[1]] +=1\n",
    "        #w[ss[0], ss[1]] += 1\n",
    "#     if not tj_r.has_key(ss[0]):\n",
    "    if ss[0] not in tj_r:\n",
    "        tj_r[ss[0]]=set()\n",
    "    tj_r[ss[0]].add((ss[1]))\n",
    "\n",
    "standard_point=[]\n",
    "\n",
    "\n",
    "for item in tj_r:\n",
    "    if (len(tj_r[item])==1) and (item in tj_r[item]):\n",
    "        standard_point.append(item)\n",
    "\n",
    "for idx in range(num_g):\n",
    "    if idx not in tj_r:\n",
    "        tj_r[idx]=set()\n",
    "        tj_r[idx].add((idx))\n",
    "\n",
    "g_list=rg.gridlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_obs = {}\n",
    "for idx, row in con_tr_feature.iterrows():\n",
    "    obs = row[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "               'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6',]].values\n",
    "    gid = int(row['gid'])\n",
    "    match = em_pro[(em_pro['BSID']==obs[0]) & (em_pro['rss_level_1']==obs[1]) & \n",
    "                      (em_pro['BSID2']==obs[2]) & (em_pro['rss_level_2']==obs[3]) & \n",
    "                      (em_pro['BSID3']==obs[4]) & (em_pro['rss_level_3']==obs[5]) & \n",
    "                      (em_pro['BSID4']==obs[6]) & (em_pro['rss_level_4']==obs[7]) & \n",
    "                      (em_pro['BSID5']==obs[8]) & (em_pro['rss_level_5']==obs[9]) & \n",
    "                      (em_pro['BSID6']==obs[10]) & (em_pro['rss_level_6']==obs[11])]\n",
    "    if match.shape[0]>0:\n",
    "        obs_idx = (gid, int(match.index[0]))\n",
    "        if obs_idx not in g_obs:\n",
    "            g_obs[obs_idx]=1\n",
    "        else:\n",
    "            g_obs[obs_idx] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(te_pred, cof_list, o_list, idx):\n",
    "    i=0\n",
    "    te_pred_n=[]\n",
    "    while i < cof_list.shape[0]:\n",
    "        if cof_list[i]==0:\n",
    "            pred_temp = te_pred[i]\n",
    "            if pred_temp in standard_point:\n",
    "                te_pred_n.append(te_pred[i])\n",
    "            else:\n",
    "                repair_r = list(w[te_pred[i], :])\n",
    "                max_idx = repair_r.index(np.max(repair_r))\n",
    "                if (max_idx, o_list[i]) in g_obs:\n",
    "                    te_pred_n.append(int(max_idx))\n",
    "                else:\n",
    "                    te_pred_n.append(te_pred[i])\n",
    "                \n",
    "        else:\n",
    "            pred_temp = te_pred[i]\n",
    "            if pred_temp in standard_point:\n",
    "                te_pred_n.append(te_pred[i])\n",
    "            else:\n",
    "                if i< cof_list.shape[0]-1:\n",
    "                    div_x=math.fabs(g_list[te_pred[i]][0]-g_list[te_pred[i+1]][0])\n",
    "                    div_y=math.fabs(g_list[te_pred[i]][1]-g_list[te_pred[i+1]][1])\n",
    "                    if sq(div_x)+sq(div_y)>2:\n",
    "                        te_pred_n.append(t_gr_list[idx][i])\n",
    "                       \n",
    "                    else:\n",
    "                        te_pred_n.append(te_pred[i])\n",
    "        i+=1\n",
    " \n",
    "    return te_pred_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "error_list = []\n",
    "error_new =[]\n",
    "i=0\n",
    "for row, time, raw, gt, in zip (pred_list, pred_list_t_idx, p_g_list, t_g_list):\n",
    "    \n",
    "    o_seq = []\n",
    "    for obs in row:\n",
    "        #print obs\n",
    "        match = em_pro[(em_pro['BSID']==obs[0]) & (em_pro['rss_level_1']==obs[1]) & \n",
    "                      (em_pro['BSID2']==obs[2]) & (em_pro['rss_level_2']==obs[3]) & \n",
    "                      (em_pro['BSID3']==obs[4]) & (em_pro['rss_level_3']==obs[5]) & \n",
    "                      (em_pro['BSID4']==obs[6]) & (em_pro['rss_level_4']==obs[7]) & \n",
    "                      (em_pro['BSID5']==obs[8]) & (em_pro['rss_level_5']==obs[9]) & \n",
    "                      (em_pro['BSID6']==obs[10]) & (em_pro['rss_level_6']==obs[11])]\n",
    "        o_seq.append(match.index[0])\n",
    "    pred = raw\n",
    "    true = gt\n",
    "    #print gt\n",
    "    \n",
    "    cof_list = hmm_viterbi(st_mat, init_prob, emit_prob, o_seq, time)\n",
    "    #print 'hmm'\n",
    "    r_pred = repair(pred, cof_list, o_seq, i)\n",
    "    \n",
    "   \n",
    "    te_predp = np.array([rg.grid_center[idx] for idx in r_pred])\n",
    "    #tps = np.array([rg.grid_center[idx] for idx in true])\n",
    "   \n",
    "    error_tep = [distance(pt1, pt2) for pt1, pt2 in zip(te_predp, true)]\n",
    "    #print np.mean(error_trp), np.mean(error_tep)\n",
    "   \n",
    "    for t in error_tep:\n",
    "        #print t\n",
    "        error_new.append(t)\n",
    "    #print i\n",
    "    i+=1\n",
    "    #print pred\n",
    "    #print 'a', r_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "err= sorted(error_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21.199999999999999, 28.134538152610443, 57.2)\n"
     ]
    }
   ],
   "source": [
    "print(np.median(error_new), np.mean(error_new), err[int(len(err) * 0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
