{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分区域\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as Math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def compute_time_interval(start, end):\n",
    "   \n",
    "    start = datetime.datetime.fromtimestamp(start / 1000.0)\n",
    "    end = datetime.datetime.fromtimestamp(end / 1000.0)\n",
    "    \n",
    "    # 相减得到秒数\n",
    "    seconds = (end- start).seconds\n",
    "    \n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = 6378137\n",
    "rj = 6356725\n",
    "from math import atan, cos, asin, sqrt, pow, pi, sin\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0\n",
    "\n",
    "def azimuth(pt_a, pt_b):\n",
    "    lon_a, lat_a = pt_a\n",
    "    lon_b, lat_b = pt_b\n",
    "    rlon_a, rlat_a = rad(lon_a), rad(lat_a)\n",
    "    rlon_b, rlat_b = rad(lon_b), rad(lat_b)\n",
    "    ec=rj+(rc-rj)*(90.-lat_a)/90.\n",
    "    ed=ec*cos(rlat_a)\n",
    "\n",
    "    dx = (rlon_b - rlon_a) * ec\n",
    "    dy = (rlat_b - rlat_a) * ed\n",
    "    if dy == 0:\n",
    "        angle = 90. \n",
    "    else:\n",
    "        angle = atan(abs(dx / dy)) * 180.0 / pi\n",
    "    dlon = lon_b - lon_a\n",
    "    dlat = lat_b - lat_a\n",
    "    if dlon > 0 and dlat <= 0:\n",
    "        angle = (90. - angle) + 90\n",
    "    elif dlon <= 0 and dlat < 0:\n",
    "        angle = angle + 180 \n",
    "    elif dlon < 0 and dlat >= 0:\n",
    "        angle = (90. - angle) + 270 \n",
    "    return angle\n",
    "\n",
    "def distance(true_pt, pred_pt):\n",
    "    lat1 = float(true_pt[1])\n",
    "    lng1 = float(true_pt[0])\n",
    "    lat2 = float(pred_pt[1])\n",
    "    lng2 = float(pred_pt[0])\n",
    "    radLat1 = rad(lat1)\n",
    "    radLat2 = rad(lat2)\n",
    "    a = radLat1 - radLat2\n",
    "    b = rad(lng1) - rad(lng2)\n",
    "    s = 2 * Math.asin(Math.sqrt(Math.pow(Math.sin(a/2),2) +\n",
    "    Math.cos(radLat1)*Math.cos(radLat2)*Math.pow(Math.sin(b/2),2)))\n",
    "    s = s * 6378.137\n",
    "    s = round(s * 10000) / 10\n",
    "    return s\n",
    "\n",
    "def sq(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_new = [\n",
    "    #'Num_connected',\n",
    "    'TrajID',\n",
    "    'ECGI_1',\n",
    "    'Dbm_1',\n",
    "    'Freq_1',\n",
    "    'CellID_1',\n",
    "    'ECGI_2',\n",
    "    'Dbm_2',\n",
    "    'Freq_2',\n",
    "    'CellID_2',\n",
    "    'ECGI_3',\n",
    "    'Dbm_3',\n",
    "    'Freq_3',\n",
    "    'CellID_3',\n",
    "    'ECGI_4',\n",
    "    'Dbm_4',\n",
    "    'Freq_4',\n",
    "    'CellID_4',\n",
    "    'ECGI_5',\n",
    "    'Dbm_5',\n",
    "    'Freq_5',\n",
    "    'CellID_5',\n",
    "    'ECGI_6',\n",
    "    'Dbm_6',\n",
    "    'Freq_6',\n",
    "    'CellID_6',\n",
    "    'ECGI_7',\n",
    "    'Dbm_7',\n",
    "    'Freq_7',\n",
    "    'CellID_7',\n",
    "    #'RSSI_6',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_rf = [\n",
    "    'ECGI_1',\n",
    "    'Dbm_1',\n",
    "    'Freq_1',\n",
    "    'CellID_1',\n",
    "    'ECGI_2',\n",
    "    'Dbm_2',\n",
    "    'Freq_2',\n",
    "    'CellID_2',\n",
    "    'ECGI_3',\n",
    "    'Dbm_3',\n",
    "    'Freq_3',\n",
    "    'CellID_3',\n",
    "    'ECGI_4',\n",
    "    'Dbm_4',\n",
    "    'Freq_4',\n",
    "    'CellID_4',\n",
    "    'ECGI_5',\n",
    "    'Dbm_5',\n",
    "    'Freq_5',\n",
    "    'CellID_5',\n",
    "    'ECGI_6',\n",
    "    'Dbm_6',\n",
    "    'Freq_6',\n",
    "    'CellID_6',\n",
    "    'ECGI_7',\n",
    "    'Dbm_7',\n",
    "    'Freq_7',\n",
    "    'CellID_7',\n",
    "    'Lon','Lat','Lon2','Lat2','Lon3','Lat3','Lon4','Lat4','Lon5','Lat5','Lon6','Lat6','Lon7','Lat7'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_2g_engpara():\n",
    "    eng_para = pd.read_csv('huawei_big_data_gongcan_4g.csv', encoding='utf-8')\n",
    "    eng_para = eng_para[['ECGI', 'PCI', '1F_UARFCN',u'经度',u'维度']]\n",
    "    eng_para = eng_para.drop_duplicates()\n",
    "    eng_para.rename(columns={u'经度': 'Lon', u'维度': 'Lat'}, inplace=True)\n",
    "    eng_para['BSID'] = range(len(eng_para))\n",
    "    eng_para['BSID'] = eng_para['BSID'].map(lambda x: x + 1)\n",
    "    return eng_para\n",
    "\n",
    "def make_rf_dataset(data, eng_para):\n",
    "    for i in range(1, 8):\n",
    "        data = data.merge(eng_para, left_on=['ECGI_%d' % i, 'Freq_%d' % i, 'CellID_%d' % i], right_on=['ECGI','1F_UARFCN','PCI'], \n",
    "                          how='left', suffixes=('', '%d' % i))\n",
    "        temp=data['CellID_%d'% i].tolist()\n",
    "        new=list()\n",
    "        for item in temp:\n",
    "            if math.isnan(item):\n",
    "                new.append(0)\n",
    "            elif int(item)<=0:\n",
    "                new.append(0)\n",
    "            else:\n",
    "                new.append(item)\n",
    "        data['CellID_%d' % i]=new\n",
    "    data = data.fillna(-999.)\n",
    "    #print data.columns\n",
    "    \n",
    "    feature = data[col_name_new+['MRTime','Timestamp','BSID','BSID2','BSID3','BSID4','BSID5','BSID6','BSID7','Longitude', 'Latitude',\n",
    "                                 'Lon','Lat','Lon2','Lat2','Lon3','Lat3','Lon4','Lat4','Lon5','Lat5','Lon6','Lat6','Lon7','Lat7']]\n",
    "   \n",
    "    \n",
    "    subset=[u'Longitude', u'Latitude', \n",
    "       u'ECGI_1', u'CellID_1',u'Freq_1',\n",
    "       u'ECGI_2', u'CellID_2',u'Freq_2',\n",
    "       u'ECGI_3', u'CellID_3',u'Freq_3',\n",
    "       u'ECGI_4', u'CellID_4',u'Freq_4',\n",
    "       u'ECGI_5', u'CellID_5',u'Freq_5',\n",
    "       u'ECGI_6', u'CellID_6',u'Freq_6',\n",
    "       u'ECGI_7', u'CellID_7',u'Freq_7',\n",
    "       ]\n",
    "    #feature=feature.drop_duplicates(subset=subset) \n",
    "    label = feature[['Longitude', 'Latitude']]\n",
    "    feature= feature.drop(['Longitude', 'Latitude'],axis=1)\n",
    "    \n",
    "    return feature, label\n",
    "\n",
    "#eng_para = merge_2g_engpara()\n",
    "eng_para =merge_2g_engpara()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_model_label(error):\n",
    "    conf_l=list()\n",
    "   \n",
    "    for t in error:\n",
    "        if t<=50:\n",
    "            conf_l.append(1)\n",
    "        else:\n",
    "            conf_l.append(0)\n",
    "    \n",
    "    return conf_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2g=pd.read_csv(\"new_school.csv\")\n",
    "data_2g = data_2g.drop(data_2g[data_2g['sub_area']==9].index)\n",
    "data_2g = data_2g.groupby('sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train, label, tr_feature_r, te_feature_r, tr_label_, te_label_ = [[]]*9, [[]]*9, [[]]*9, [[]]*9, [[]]*9, [[]]*9\n",
    "\n",
    "for name, group in data_2g:\n",
    "    i = int(name)\n",
    "    train[i], label[i] = make_rf_dataset(group, eng_para)\n",
    "    tr_feature_r[i], te_feature_r[i], tr_label_[i], te_label_[i] = train_test_split(train[i], label[i], test_size=0.4,random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_pre(data):\n",
    "    data=data.iloc[:,1:]\n",
    "    label=data[['Longitude','Latitude']]\n",
    "    data=data.drop(['Longitude', 'Latitude'],axis=1)\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_tr_feature, con_te_feature, con_tr_p, con_te_p = [[]]*9, [[]]*9, [[]]*9, [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    con_tr_feature[i], con_te_feature[i], con_tr_p[i], con_te_p[i] = train_test_split(te_feature_r[i], te_label_[i], test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con_tr_feature.to_csv(\"2g/conf_tr_jd2g.csv\")\n",
    "#con_te_feature.to_csv(\"2g/conf_te_jd2g.csv\")\n",
    "#tr_feature_r.to_csv(\"2g/total_conf_tr_jd2g.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid\n",
    "\n",
    "rg, tr_label_g, con_tr_j, con_te_j = [[]]*9, [[]]*9, [[]]*9, [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    rg[i] = grid.RoadGrid(np.vstack((tr_label_[i].values, te_label_[i].values)),100)\n",
    "    tr_label_g[i] = rg[i].transform(tr_label_[i].values, False) # label所在的grid\n",
    "    #rint tr_label_\n",
    "    con_tr_j[i] = rg[i].transform(con_tr_p[i].values, False)\n",
    "    con_te_j[i] = rg[i].transform(con_te_p[i].values, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr, tr_pred1, tr_con_pred, te_con_pred = [[]]*9, [[]]*9, [[]]*9, [[]]*9\n",
    "error_tr, error_te = [], []\n",
    "\n",
    "for i in range(9):\n",
    "    est=RandomForestClassifier( n_jobs=-1,\n",
    "        n_estimators =50,\n",
    "        max_features='sqrt'\n",
    "    ).fit(tr_feature_r[i][col_name_rf].values, tr_label_g[i])\n",
    "\n",
    "    pred_tr[i]=est.predict(tr_feature_r[i][col_name_rf].values)\n",
    "    tr_pred1[i] = np.array([rg[i].grid_center[idx] for idx in pred_tr[i]])\n",
    "    error_tr = [distance(pt1, pt2) for pt1, pt2 in zip(tr_pred1[i], tr_label_[i].values)]\n",
    "\n",
    "    pred_con_tr=est.predict(con_tr_feature[i][col_name_rf].values)\n",
    "    pred_con_te=est.predict(con_te_feature[i][col_name_rf].values)\n",
    "    tr_con_pred[i] = np.array([rg[i].grid_center[idx] for idx in pred_con_tr])\n",
    "    te_con_pred[i] = np.array([rg[i].grid_center[idx] for idx in pred_con_te])\n",
    "    error_con_tr = [distance(pt1, pt2) for pt1, pt2 in zip(tr_con_pred[i], con_tr_p[i].values)]\n",
    "    error_con_te = [distance(pt1, pt2) for pt1, pt2 in zip(te_con_pred[i], con_te_p[i].values)]\n",
    "    error_tr += error_con_tr\n",
    "    error_te += error_con_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47.600000000000001, 90.341951933519056, 212.7)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_te = sorted(error_te)\n",
    "np.median(error_te), np.mean(error_te), error_te[int(len(error_te) * 0.90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(feature, pred, timestamp):\n",
    "    add_feature = []\n",
    "    timestamp_new=np.array(timestamp)\n",
    "    for i in range(0, len(pred)):\n",
    "        if i == 0:\n",
    "            last_pt = pred[i]\n",
    "            last_time = timestamp_new[i]\n",
    "        else:\n",
    "            last_pt = pred[i-1]\n",
    "            last_time = timestamp_new[i-1]\n",
    "        if i == len(pred)-1:\n",
    "            next_pt = pred[i]\n",
    "            next_time = timestamp_new[i]\n",
    "        else:\n",
    "            next_pt = pred[i+1]\n",
    "            next_time = timestamp_new[i+1]\n",
    "        sub_add_feature = []\n",
    "        sub_add_feature.append(distance(last_pt, pred[i]))\n",
    "        sub_add_feature.append(distance(next_pt, pred[i]))\n",
    "        sub_add_feature.append(azimuth(last_pt, pred[i]))\n",
    "        sub_add_feature.append(azimuth(pred[i], next_pt))\n",
    "        sub_add_feature.append(timestamp_new[i]-last_time if timestamp_new[i]-last_time > 0 else 0)\n",
    "        sub_add_feature.append(next_time-timestamp_new[i] if next_time-timestamp_new[i] > 0 else 0)\n",
    "        sub_add_feature.append(sub_add_feature[0] / sub_add_feature[4] if sub_add_feature[4] > 0 else 0)\n",
    "        sub_add_feature.append(sub_add_feature[1] / sub_add_feature[5] if sub_add_feature[5] > 0 else 0)\n",
    "        sub_add_feature.append(last_pt[0])\n",
    "        sub_add_feature.append(last_pt[1])\n",
    "        sub_add_feature.append(next_pt[0])\n",
    "        sub_add_feature.append(next_pt[1])\n",
    "        sub_add_feature.append(pred[i][0])\n",
    "        sub_add_feature.append(pred[i][1])\n",
    "        #sub_add_feature.append(grid[i])\n",
    "        add_feature.append(sub_add_feature)\n",
    "    add_feature = np.asarray(add_feature)\n",
    "    features = np.array(np.hstack((feature, add_feature)), dtype=float)\n",
    "    feature = csc_matrix(features)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred, te_pred, pred_con_tr, pred_con_te, error_tr, error_te = [[]]*9, [[]]*9, [[]]*9, [[]]*9, [[]]*9, [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    feature_tr = feature_engineer(tr_feature_r[i][col_name_rf], tr_pred1[i], tr_feature_r[i]['Timestamp'])\n",
    "    feature_con_tr = feature_engineer(con_tr_feature[i][col_name_rf], tr_con_pred[i], con_tr_feature[i]['Timestamp'])\n",
    "    feature_con_te = feature_engineer(con_te_feature[i][col_name_rf], te_con_pred[i], con_te_feature[i]['Timestamp'])\n",
    "\n",
    "    est1=RandomForestClassifier( n_jobs=-1,\n",
    "        n_estimators =50,\n",
    "        max_features='sqrt'\n",
    "    ).fit(feature_tr, tr_label_g[i])\n",
    "\n",
    "    pred_con_tr[i]=est1.predict(feature_con_tr)\n",
    "    pred_con_te[i]=est1.predict(feature_con_te)\n",
    "    tr_pred[i] = np.array([rg[i].grid_center[idx] for idx in pred_con_tr[i]])\n",
    "    te_pred[i] = np.array([rg[i].grid_center[idx] for idx in pred_con_te[i]])\n",
    "    error_tr[i] = [distance(pt1, pt2) for pt1, pt2 in zip(tr_pred[i], con_tr_p[i].values)]\n",
    "    error_te[i] = [distance(pt1, pt2) for pt1, pt2 in zip(te_pred[i], con_te_p[i].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    tr_feature_r[i]['Longitude'] = tr_label_[i].iloc[:, 0].values\n",
    "    tr_feature_r[i]['Latitude'] = tr_label_[i].iloc[:, 1].values\n",
    "    tr_feature_r[i]['gid'] = tr_label_g[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_label_tr, conf_label_te = [[]]*9, [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    conf_label_tr[i] = conf_model_label(error_tr[i])\n",
    "    conf_label_te[i] = conf_model_label(error_te[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    con_tr_feature[i][\"conf\"]= conf_label_tr[i]\n",
    "    con_te_feature[i][\"conf\"]= conf_label_te[i]\n",
    "    con_tr_feature[i][\"error\"]= error_tr[i]\n",
    "    con_te_feature[i][\"error\"]= error_te[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    con_tr_feature[i]['Longitude'] = con_tr_p[i].iloc[:,0]\n",
    "    con_tr_feature[i]['Latitude'] = con_tr_p[i].iloc[:,1]\n",
    "    con_te_feature[i]['Longitude'] = con_te_p[i].iloc[:,0]\n",
    "    con_te_feature[i]['Latitude'] = con_te_p[i].iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    con_tr_feature[i]['p_gid'] = pred_con_tr[i]\n",
    "    con_tr_feature[i]['gid'] = con_tr_j[i]\n",
    "    con_te_feature[i]['p_gid'] = pred_con_te[i]\n",
    "    con_te_feature[i]['gid'] = con_te_j[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss_level(dbm):\n",
    "    if dbm>-50:\n",
    "        return 1\n",
    "    elif dbm >-60:\n",
    "        return 2\n",
    "    elif dbm >-70:\n",
    "        return 3\n",
    "    elif dbm>-80:\n",
    "        return 4\n",
    "    elif dbm>-90:\n",
    "        return 5\n",
    "    elif dbm>-100:\n",
    "        return 6\n",
    "    elif dbm>-110:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    for j in range(1, 8):\n",
    "        con_tr_feature[i]['rss_level_%d' % j] = con_tr_feature[i]['Dbm_%d' % j].map(lambda x: rss_level(x))  \n",
    "        con_te_feature[i]['rss_level_%d' % j] = con_te_feature[i]['Dbm_%d' % j].map(lambda x: rss_level(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob, total_ob_bs, total_ob_rss, total_ob_te = [[]]*9, [[]]*9, [[]]*9, [[]]*9, \n",
    "\n",
    "for i in range(9):\n",
    "    total_ob[i] = con_tr_feature[i][['BSID','rss_level_1','BSID2','rss_level_2','BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "                   'BSID5','rss_level_5','BSID6','rss_level_6','BSID7','rss_level_7']].drop_duplicates()\n",
    "    total_ob_bs[i] = con_tr_feature[i][['BSID','BSID2','BSID3','BSID4','BSID5','BSID6','BSID7']].drop_duplicates()\n",
    "    total_ob_rss[i] = con_tr_feature[i][['rss_level_1','rss_level_2','rss_level_3','rss_level_4','rss_level_5','rss_level_6',\n",
    "                               'rss_level_7']].drop_duplicates()\n",
    "    total_ob_te[i] = con_te_feature[i][['BSID','rss_level_1','BSID2','rss_level_2','BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6','BSID7','rss_level_7']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(list1, list2):\n",
    "    #print list1, list2\n",
    "    union_set = len(set(list1)|set(list2))#并集长度\n",
    "    intersection_set = len(set(list1)&set(list2))#交集长度\n",
    "\n",
    "    Jaccard = float(intersection_set/union_set) #Jaccar\n",
    "    return Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_emission_pro(jd_list, match, bs_list, ss_list, can_list, total_c, conf, i):\n",
    "    pro_list = []\n",
    "    weight_list = []\n",
    "    if match.shape[0]>0:\n",
    "        weight_list.append(math.log(1+match.shape[0])*1)\n",
    "        match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "             & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5]) & (match['rss_level_7']==ss_list[6])]\n",
    "        if match_ss.shape[0]>0:\n",
    "            pro_list.append(float(match_ss[match_ss['conf']==conf].shape[0])/float(total_c))\n",
    "        else:\n",
    "            pro_list.append(float(match[match['conf']==conf].shape[0])/float(total_c))\n",
    "        \n",
    "        \n",
    "    for can_temp, jd in zip(can_list, jd_list):\n",
    "        match_c = con_tr_feature[i][(con_tr_feature[i]['BSID']==int(can_temp[0])) & (con_tr_feature[i]['BSID2']==int(can_temp[1]))\n",
    "                      &(con_tr_feature[i]['BSID3']==int(can_temp[2])) & (con_tr_feature[i]['BSID4']==int(can_temp[3]))\n",
    "                      &(con_tr_feature[i]['BSID5']==int(can_temp[4])) & (con_tr_feature[i]['BSID6']==int(can_temp[5]))\n",
    "                                 & (con_tr_feature[i]['BSID7']==int(can_temp[6]))]\n",
    "        count = match_c.shape[0]\n",
    "        weight_list.append(math.log(1+count)*jd)\n",
    "        \n",
    "        match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                            & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                            & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])\n",
    "                            & (match_c['rss_level_7']==ss_list[6])]\n",
    "        if match_ss_c.shape[0]>0:\n",
    "            pro_list.append(float(match_ss_c[match_ss_c['conf']==conf].shape[0])/float(total_c))\n",
    "        else:\n",
    "            pro_list.append(float(match_c[match_c['conf']==conf].shape[0])/float(total_c))\n",
    "\n",
    "\n",
    "    weight_sum = np.sum(weight_list)\n",
    "    ad_em_po = 0\n",
    "    \n",
    "    for x, y in zip(pro_list, weight_list):\n",
    "        ad_em_po += x * (y / weight_sum)\n",
    "    \n",
    "    return ad_em_po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2647, 14)\n",
      "(2039, 14)\n",
      "(620, 14)\n",
      "(1251, 14)\n",
      "(2551, 14)\n",
      "(2488, 14)\n",
      "(2735, 14)\n",
      "(2057, 14)\n",
      "(353, 14)\n"
     ]
    }
   ],
   "source": [
    "zeros_list = [[]]*9\n",
    "\n",
    "for k in range(9):\n",
    "    zeros_list[k] = []\n",
    "    i =0\n",
    "    j=0\n",
    "    zero_num = con_tr_feature[k][con_tr_feature[k]['conf']==0].shape[0]\n",
    "    print total_ob_te[k].shape\n",
    "    for idx, row in total_ob_te[k].iterrows():\n",
    "        bs_list =row[['BSID','BSID2','BSID3','BSID4','BSID5','BSID6','BSID7']].values\n",
    "        ss_list = row[['rss_level_1','rss_level_2','rss_level_3','rss_level_4','rss_level_5','rss_level_6','rss_level_7']].values\n",
    "        match = con_tr_feature[k][(con_tr_feature[k]['BSID']==int(bs_list[0])) & (con_tr_feature[k]['BSID2']==int(bs_list[1]))\n",
    "                              &(con_tr_feature[k]['BSID3']==int(bs_list[2])) & (con_tr_feature[k]['BSID4']==int(bs_list[3]))\n",
    "                              &(con_tr_feature[k]['BSID5']==int(bs_list[4])) & (con_tr_feature[k]['BSID6']==int(bs_list[5]))\n",
    "                              &(con_tr_feature[k]['BSID7']==int(bs_list[6]))]\n",
    "        if match.shape[0]<5:\n",
    "            can_bs_row = []\n",
    "            can_j = []\n",
    "            jaccd_max = []\n",
    "            idx_list = []\n",
    "            for idxx, roww in total_ob_bs[k].iterrows():\n",
    "                can_bs_list = roww.values\n",
    "                jd = jaccard_sim(bs_list, can_bs_list)\n",
    "                jaccd_max.append(jd)\n",
    "                idx_list.append(idxx)\n",
    "                if jd>0.5:\n",
    "                    can_bs_row.append(can_bs_list)\n",
    "                    can_j.append(jd)\n",
    "\n",
    "            if len(can_bs_row)==0:\n",
    "                can_idx = jaccd_max.index(np.max(jaccd_max))\n",
    "                can_temp = total_ob_bs[k].iloc[can_idx,:].values\n",
    "                can_bs_row.append(total_ob_bs[k].iloc[can_idx,:].values)\n",
    "                match_c = con_tr_feature[k][(con_tr_feature[k]['BSID']==int(can_temp[0])) & (con_tr_feature[k]['BSID2']==int(can_temp[1]))\n",
    "                              &(con_tr_feature[k]['BSID3']==int(can_temp[2])) & (con_tr_feature[k]['BSID4']==int(can_temp[3]))\n",
    "                              &(con_tr_feature[k]['BSID5']==int(can_temp[4])) & (con_tr_feature[k]['BSID6']==int(can_temp[5]))\n",
    "                              &(con_tr_feature[k]['BSID7']==int(can_temp[6]))]\n",
    "\n",
    "                match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                                & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                                & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])\n",
    "                                & (match_c['rss_level_7']==ss_list[6])]\n",
    "                if match_ss_c.shape[0]>0:\n",
    "                    zeros_list[k].append(float(match_ss_c[match_ss_c['conf']==0].shape[0])/float(zero_num))\n",
    "                else:\n",
    "                    zeros_list[k].append(float(match_c[match_c['conf']==0].shape[0])/float(zero_num))\n",
    "\n",
    "                j+=1 \n",
    "            else:\n",
    "                zeros_list[k].append(adaptive_emission_pro(can_j, match, bs_list, ss_list, can_bs_row, zero_num, 0, k))\n",
    "            #print i, len(can_bs_row)\n",
    "            i+=1\n",
    "        else:\n",
    "            j+=1\n",
    "            match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "                 & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5]) & (match['rss_level_7']==ss_list[6])]\n",
    "            if match_ss.shape[0]>0:\n",
    "                zeros_list[k].append(float(match_ss[match_ss['conf']==0].shape[0])/float(zero_num))\n",
    "            else:\n",
    "                zeros_list[k].append(float(match[match['conf']==0].shape[0])/float(zero_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_list2 = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "#     zeros_list2[i] = np.array(zeros_list[i])\n",
    "#     np.savetxt('zeros_list'+str(i)+'.txt', zeros_list2[i])\n",
    "    zeros_list2[i] = np.loadtxt('zeros_list'+str(i)+'.txt')\n",
    "    zeros_list2[i] = zeros_list2[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2647, 14) 2647\n",
      "(2039, 14) 2039\n",
      "(620, 14) 620\n",
      "(1251, 14) 1251\n",
      "(2551, 14) 2551\n",
      "(2488, 14) 2488\n",
      "(2735, 14) 2735\n",
      "(2057, 14) 2057\n",
      "(353, 14) 353\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print total_ob_te[i].shape, len(zeros_list2[i])\n",
    "    total_ob_te[i]['conf_ad_em_pro_0'] = zeros_list2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_list = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    one_list[i] = []\n",
    "    one_num = con_tr_feature[i][con_tr_feature[i]['conf']==1].shape[0]\n",
    "    for idx, row in total_ob_te[i].iterrows():\n",
    "        bs_list =row[['BSID','BSID2','BSID3','BSID4','BSID5','BSID6','BSID7']].values\n",
    "        ss_list = row[['rss_level_1','rss_level_2','rss_level_3','rss_level_4','rss_level_5','rss_level_6','rss_level_7']].values\n",
    "        #print bs_list[0]\n",
    "        match = con_tr_feature[i][(con_tr_feature[i]['BSID']==int(bs_list[0])) & (con_tr_feature[i]['BSID2']==int(bs_list[1]))\n",
    "                              &(con_tr_feature[i]['BSID3']==int(bs_list[2])) & (con_tr_feature[i]['BSID4']==int(bs_list[3]))\n",
    "                              &(con_tr_feature[i]['BSID5']==int(bs_list[4])) & (con_tr_feature[i]['BSID6']==int(bs_list[5]))\n",
    "                              &(con_tr_feature[i]['BSID7']==int(bs_list[6]))]\n",
    "        if match.shape[0]<5:\n",
    "            can_bs_row = []\n",
    "            can_j = []\n",
    "            jaccd_max = []\n",
    "            idx_list = []\n",
    "            for idxx, roww in total_ob_bs[i].iterrows():\n",
    "                can_bs_list = roww.values\n",
    "                jd = jaccard_sim(bs_list, can_bs_list)\n",
    "                jaccd_max.append(jd)\n",
    "                idx_list.append(idxx)\n",
    "                if jd>0.5:\n",
    "                    can_bs_row.append(can_bs_list)\n",
    "                    can_j.append(jd)\n",
    "\n",
    "            if len(can_bs_row)==0:\n",
    "                can_idx = jaccd_max.index(np.max(jaccd_max))\n",
    "                can_temp = total_ob_bs[i].iloc[can_idx,:].values\n",
    "                can_bs_row.append(total_ob_bs[i].iloc[can_idx,:].values)\n",
    "                match_c = con_tr_feature[i][(con_tr_feature[i]['BSID']==int(can_temp[0])) & (con_tr_feature[i]['BSID2']==int(can_temp[1]))\n",
    "                              &(con_tr_feature[i]['BSID3']==int(can_temp[2])) & (con_tr_feature[i]['BSID4']==int(can_temp[3]))\n",
    "                              &(con_tr_feature[i]['BSID5']==int(can_temp[4])) & (con_tr_feature[i]['BSID6']==int(can_temp[5]))\n",
    "                              &(con_tr_feature[i]['BSID7']==int(can_temp[6]))]\n",
    "\n",
    "                match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                                & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                                & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])\n",
    "                                & (match_c['rss_level_7']==ss_list[6])]\n",
    "                if match_ss_c.shape[0]>0:\n",
    "                    one_list[i].append(float(match_ss_c[match_ss_c['conf']==1].shape[0])/float(one_num))\n",
    "                else:\n",
    "                    one_list[i].append(float(match_c[match_c['conf']==1].shape[0])/float(one_num))\n",
    "\n",
    "            else:\n",
    "                one_list[i].append(adaptive_emission_pro(can_j, match, bs_list, ss_list, can_bs_row, one_num, 1, i))\n",
    "\n",
    "        else:\n",
    "\n",
    "            match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "                 & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5]) & (match['rss_level_7']==ss_list[6])]\n",
    "            if match_ss.shape[0]>0:\n",
    "                one_list[i].append(float(match_ss[match_ss['conf']==1].shape[0])/float(one_num))\n",
    "            else:\n",
    "                one_list[i].append(float(match[match['conf']==1].shape[0])/float(one_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_list2 = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "#     one_list2[i] = np.array(one_list[i])\n",
    "#     np.savetxt('one_list'+str(i)+'.txt', one_list2[i])\n",
    "    one_list2[i] = np.loadtxt('one_list'+str(i)+'.txt')\n",
    "    one_list2[i] = one_list2[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    total_ob_te[i]['conf_ad_em_pro_1'] = one_list2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    con_te_feature[i]['gid'] = con_te_j[i]\n",
    "    con_te_feature[i]['p_gid'] = pred_con_te[i]\n",
    "    con_tr_feature[i]['gid'] = con_tr_j[i]\n",
    "    con_tr_feature[i]['p_gid'] = pred_con_tr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_tr = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    trajs_tr[i] = con_tr_feature[i].groupby(['TrajID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list, st_mat = [[]]*9, [[]]*9\n",
    "\n",
    "for k in range(9):\n",
    "    time_list[k] = []\n",
    "    st_mat[k] = np.zeros((14,2,2)) #0-0 0-1 1-0, 1-1\n",
    "    for trajid, traj in trajs_tr[k]:\n",
    "        traj = traj.sort_values(by=['Timestamp'],ascending=True)\n",
    "        t_time = traj['Timestamp'].values\n",
    "        conf = traj['conf'].values\n",
    "        for i in range(traj.shape[0]-1):\n",
    "            time_list[k].append(compute_time_interval(t_time[i], t_time[i + 1]))\n",
    "            idx = int(compute_time_interval(t_time[i], t_time[i + 1])/5)\n",
    "            if idx >12:\n",
    "                idx = 13\n",
    "            if conf[i]==0 and conf[i+1]==0:\n",
    "                st_mat[k][idx, 0, 0] +=1\n",
    "            if conf[i] ==0 and conf[i+1] ==1:\n",
    "                st_mat[k][idx, 0, 1] +=1\n",
    "            if conf[i]==1 and conf[i+1]==0:\n",
    "                st_mat[k][idx, 1, 0] +=1\n",
    "            if conf[i] ==1 and conf[i+1] ==1:\n",
    "                st_mat[k][idx, 1, 1] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_te = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    trajs_te[i] = con_te_feature[i].groupby(['TrajID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_t_idx, pred_list, p_g_list, t_g_list = [[]]*9, [[]]*9, [[]]*9, [[]]*9\n",
    "\n",
    "for k in range(9):\n",
    "    pred_list_t_idx[k] = []\n",
    "    #st_mat = np.zeros((14,2,2)) #0-0 0-1 1-0, 1-1\n",
    "    pred_list[k] = []\n",
    "    p_g_list[k] = []\n",
    "    t_g_list[k] = []\n",
    "    for trajid, traj in trajs_te[k]:\n",
    "        traj = traj.sort_values(by=['Timestamp'],ascending=True)\n",
    "        t_time = traj['Timestamp'].values\n",
    "        conf = traj['conf'].values\n",
    "        idx_list = []\n",
    "        for i in range(traj.shape[0]-1):\n",
    "            time_list.append(compute_time_interval(t_time[i], t_time[i + 1]))\n",
    "            idx = int(compute_time_interval(t_time[i], t_time[i + 1])/5)\n",
    "            if idx >12:\n",
    "                idx = 13\n",
    "            idx_list.append(idx)\n",
    "        pred_list[k].append(traj[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "                   'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "                   'BSID5','rss_level_5','BSID6','rss_level_6',]].values)\n",
    "\n",
    "        pred_list_t_idx[k].append(idx_list)\n",
    "        p_g_list[k].append(traj['p_gid'].values)\n",
    "        t_g_list[k].append(traj[['Longitude','Latitude']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prob = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    init_prob[i] = [float(con_tr_feature[i][con_tr_feature[i]['conf']==0].shape[0])/con_tr_feature[i].shape[0],\n",
    "                 float(con_tr_feature[i][con_tr_feature[i]['conf']==1].shape[0])/con_tr_feature[i].shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_viterbi(st_mat, init_prob, emit_prob, obs_seq, t_list):\n",
    "    Nstate = 2\n",
    "    Nobs = int(emit_prob.shape[0])\n",
    "    T = len(obs_seq)\n",
    "    \n",
    "    partial_prob = np.zeros((Nstate,T))\n",
    "\n",
    "    path = np.zeros((Nstate,T))\n",
    "\n",
    "    for i in range(Nstate):\n",
    "        partial_prob[i,0] = init_prob[i] * emit_prob[obs_seq[0], i]\n",
    "        path[i,0] = i\n",
    "\n",
    "\n",
    "    for t in range(1,T,1):\n",
    "        newpath = np.zeros((Nstate,T))\n",
    "        for i in range(Nstate):\n",
    "            prob = -1.0\n",
    "            for j in range(Nstate):\n",
    "                nprob = partial_prob[j,t-1] * st_mat[t_list[t-1], j, i] * emit_prob[obs_seq[t], i]\n",
    "                if nprob > prob:\n",
    "                    prob = nprob\n",
    "                    partial_prob[i,t] = nprob\n",
    "                    newpath[i,0:t] = path[j,0:t]\n",
    "                    newpath[i,t] = i\n",
    "        path = newpath\n",
    "    \n",
    "    prob = -1.0\n",
    "    j = 0\n",
    "    for i in range(Nstate):\n",
    "        if(partial_prob[i,T-1] > prob):\n",
    "            prob = partial_prob[i,T-1]\n",
    "            j = i\n",
    "\n",
    "    return path[j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_pro = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    em_pro[i] = total_ob_te[i]\n",
    "    em_pro[i] = em_pro[i].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_prob = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    emit_prob[i] = em_pro[i][['conf_ad_em_pro_0', 'conf_ad_em_pro_1']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, tj_r, standard_point, g_list = [[]]*9, [[]]*9, [[]]*9, [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    num_g = rg[i].n_grid\n",
    "    w[i] = np.zeros((num_g, num_g))\n",
    "    \n",
    "    test = zip(pred_con_tr[i], con_tr_j[i], error_tr[i])\n",
    "    tj_r[i] = {}\n",
    "    for ss in test:\n",
    "        if ss[0]!=ss[1] and ss[2]>50:\n",
    "             w[i][ss[0]][ss[1]] +=1\n",
    "        if ss[0] not in tj_r[i]:\n",
    "            tj_r[i][ss[0]]=set()\n",
    "        tj_r[i][ss[0]].add((ss[1]))\n",
    "\n",
    "    standard_point[i]=[]\n",
    "\n",
    "\n",
    "    for item in tj_r[i]:\n",
    "        if (len(tj_r[i][item])==1) and (item in tj_r[i][item]):\n",
    "            standard_point[i].append(item)\n",
    "\n",
    "    for idx in range(num_g):\n",
    "        if idx not in tj_r[i]:\n",
    "            tj_r[i][idx]=set()\n",
    "            tj_r[i][idx].add((idx))\n",
    "\n",
    "    g_list[i]=rg[i].gridlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_obs = [[]]*9\n",
    "\n",
    "for i in range(9):\n",
    "    g_obs[i] = {}\n",
    "    for idx, row in con_tr_feature[i].iterrows():\n",
    "        obs = row[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "                   'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "                   'BSID5','rss_level_5','BSID6','rss_level_6',]].values\n",
    "        gid = int(row['gid'])\n",
    "        match = em_pro[i][(em_pro[i]['BSID']==obs[0]) & (em_pro[i]['rss_level_1']==obs[1]) & \n",
    "                          (em_pro[i]['BSID2']==obs[2]) & (em_pro[i]['rss_level_2']==obs[3]) & \n",
    "                          (em_pro[i]['BSID3']==obs[4]) & (em_pro[i]['rss_level_3']==obs[5]) & \n",
    "                          (em_pro[i]['BSID4']==obs[6]) & (em_pro[i]['rss_level_4']==obs[7]) & \n",
    "                          (em_pro[i]['BSID5']==obs[8]) & (em_pro[i]['rss_level_5']==obs[9]) & \n",
    "                          (em_pro[i]['BSID6']==obs[10]) & (em_pro[i]['rss_level_6']==obs[11])]\n",
    "        if match.shape[0]>0:\n",
    "            obs_idx = (gid, int(match.index[0]))\n",
    "            if obs_idx not in g_obs[i]:\n",
    "                g_obs[i][obs_idx]=1\n",
    "            else:\n",
    "                g_obs[i][obs_idx] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(te_pred, cof_list, o_list, k):\n",
    "    i=0\n",
    "    te_pred_n=[]\n",
    "    while i < len(cof_list):\n",
    "        if cof_list[i]==0:\n",
    "            pred_temp = te_pred[i]\n",
    "            if pred_temp in standard_point[k]:\n",
    "                te_pred_n.append(te_pred[i])\n",
    "            else:\n",
    "                repair_r = list(w[k][te_pred[i], :])\n",
    "                max_idx = repair_r.index(np.max(repair_r))\n",
    "                if (max_idx, o_list[i]) in g_obs[k]:\n",
    "                    te_pred_n.append(int(max_idx))\n",
    "                    #print 'ok'\n",
    "                else:\n",
    "                    \n",
    "                    te_pred_n.append(te_pred[i])\n",
    "                \n",
    "        else:\n",
    "            te_pred_n.append(te_pred[i])\n",
    "        i+=1\n",
    "            \n",
    "    #print len(te_pred_n)\n",
    "    return te_pred_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684.5\n",
      "635.0\n",
      "447.0\n",
      "778.35\n",
      "379.0\n",
      "605.1\n",
      "421.65\n",
      "626.2\n",
      "895.7\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "error = []\n",
    "\n",
    "for i in range(9):\n",
    "    error_new =[]\n",
    "    for row, time, raw, gt in zip (pred_list[i], pred_list_t_idx[i], p_g_list[i], t_g_list[i]):\n",
    "\n",
    "        o_seq = []\n",
    "        for obs in row:\n",
    "            #print obs\n",
    "            match = em_pro[i][(em_pro[i]['BSID']==obs[0]) & (em_pro[i]['rss_level_1']==obs[1]) & \n",
    "                          (em_pro[i]['BSID2']==obs[2]) & (em_pro[i]['rss_level_2']==obs[3]) & \n",
    "                          (em_pro[i]['BSID3']==obs[4]) & (em_pro[i]['rss_level_3']==obs[5]) & \n",
    "                          (em_pro[i]['BSID4']==obs[6]) & (em_pro[i]['rss_level_4']==obs[7]) & \n",
    "                          (em_pro[i]['BSID5']==obs[8]) & (em_pro[i]['rss_level_5']==obs[9]) & \n",
    "                          (em_pro[i]['BSID6']==obs[10]) & (em_pro[i]['rss_level_6']==obs[11])]\n",
    "            o_seq.append(match.index[0])\n",
    "        pred = raw\n",
    "        true = gt\n",
    "        cof_list = hmm_viterbi(st_mat[i], init_prob[i], emit_prob[i], o_seq, time)\n",
    "        r_pred = repair(pred, cof_list, o_seq, i)\n",
    "\n",
    "\n",
    "        te_predp = np.array([rg[i].grid_center[idx] for idx in r_pred])\n",
    "\n",
    "        error_tep = [distance(pt1, pt2) for pt1, pt2 in zip(te_predp, true)]\n",
    "\n",
    "        for t in error_tep:\n",
    "            error_new.append(t)  \n",
    "    print np.median(error_new)\n",
    "    error += error_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "err= sorted(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548.70000000000005, 599.79654846782194, 1097.0)\n"
     ]
    }
   ],
   "source": [
    "print(np.median(error), np.mean(error), err[int(len(err) * 0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
