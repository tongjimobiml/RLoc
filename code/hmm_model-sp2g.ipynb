{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as Math\n",
    "import matplotlib\n",
    "matplotlib.use(\"Pdf\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "import grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def compute_time_interval(start, end):\n",
    "   \n",
    "    start = datetime.datetime.fromtimestamp(start / 1000.0)\n",
    "    end = datetime.datetime.fromtimestamp(end / 1000.0)\n",
    "    \n",
    "    # 相减得到秒数\n",
    "    seconds = (end- start).seconds\n",
    "    \n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMean(x,y):\n",
    "    sum_x = sum(x)\n",
    "    sum_y = sum(y)\n",
    "    n = len(x)\n",
    "    x_mean = float(sum_x+0.0)/n\n",
    "    y_mean = float(sum_y+0.0)/n\n",
    "    return x_mean,y_mean\n",
    "\n",
    "def calcPearson(x,y):\n",
    "    x_mean,y_mean = calcMean(x,y)\t#计算x,y向量平均值\n",
    "    n = len(x)\n",
    "    sumTop = 0.0\n",
    "    sumBottom = 0.0\n",
    "    x_pow = 0.0\n",
    "    y_pow = 0.0\n",
    "    for i in range(n):\n",
    "        sumTop += (x[i]-x_mean)*(y[i]-y_mean)\n",
    "    for i in range(n):\n",
    "        x_pow += math.pow(x[i]-x_mean,2)\n",
    "    for i in range(n):\n",
    "        y_pow += math.pow(y[i]-y_mean,2)\n",
    "    sumBottom = math.sqrt(x_pow*y_pow)\n",
    "    p = sumTop/sumBottom\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "def distribution(data,low=0,up=125,r=5,bins=25):\n",
    "    p_list=np.zeros((bins))\n",
    "    i=0\n",
    "    total=len(data)\n",
    "    #print total\n",
    "    while low + r <= up:\n",
    "        for t in data:\n",
    "            if t >= low and t < low + r:\n",
    "                p_list[i] += 1\n",
    "        low += r\n",
    "        i += 1\n",
    "    p_u_list=list()\n",
    "    #print p_list\n",
    "    if len(data)>0:\n",
    "        for t in p_list:\n",
    "            #print int(t)\n",
    "            p_u_list.append(float(t)/float(total))\n",
    "    else:\n",
    "        p_u_list=p_list.tolist()\n",
    "    return p_u_list\n",
    "    \n",
    "def p_norm_distance(x,y):\n",
    "    #print x,y\n",
    "    X = np.vstack([x,y])\n",
    "    d2 = pdist(X,'minkowski',p=3)\n",
    "    return d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = 6378137\n",
    "rj = 6356725\n",
    "from math import atan, cos, asin, sqrt, pow, pi, sin\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0\n",
    "\n",
    "def azimuth(pt_a, pt_b):\n",
    "    lon_a, lat_a = pt_a\n",
    "    lon_b, lat_b = pt_b\n",
    "    rlon_a, rlat_a = rad(lon_a), rad(lat_a)\n",
    "    rlon_b, rlat_b = rad(lon_b), rad(lat_b)\n",
    "    ec=rj+(rc-rj)*(90.-lat_a)/90.\n",
    "    ed=ec*cos(rlat_a)\n",
    "\n",
    "    dx = (rlon_b - rlon_a) * ec\n",
    "    dy = (rlat_b - rlat_a) * ed\n",
    "    if dy == 0:\n",
    "        angle = 90. \n",
    "    else:\n",
    "        angle = atan(abs(dx / dy)) * 180.0 / pi\n",
    "    dlon = lon_b - lon_a\n",
    "    dlat = lat_b - lat_a\n",
    "    if dlon > 0 and dlat <= 0:\n",
    "        angle = (90. - angle) + 90\n",
    "    elif dlon <= 0 and dlat < 0:\n",
    "        angle = angle + 180 \n",
    "    elif dlon < 0 and dlat >= 0:\n",
    "        angle = (90. - angle) + 270 \n",
    "    return angle\n",
    "\n",
    "def distance(true_pt, pred_pt):\n",
    "    lat1 = float(true_pt[1])\n",
    "    lng1 = float(true_pt[0])\n",
    "    lat2 = float(pred_pt[1])\n",
    "    lng2 = float(pred_pt[0])\n",
    "    radLat1 = rad(lat1)\n",
    "    radLat2 = rad(lat2)\n",
    "    a = radLat1 - radLat2\n",
    "    b = rad(lng1) - rad(lng2)\n",
    "    s = 2 * Math.asin(Math.sqrt(Math.pow(Math.sin(a/2),2) +\n",
    "    Math.cos(radLat1)*Math.cos(radLat2)*Math.pow(Math.sin(b/2),2)))\n",
    "    s = s * 6378.137\n",
    "    s = round(s * 10000) / 10\n",
    "    return s\n",
    "\n",
    "def sq(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_new = [\n",
    "    #'Num_connected',\n",
    "    'TrajID',\n",
    "    'RNCID_1',\n",
    "    'CellID_1',\n",
    "    'AsuLevel_1',\n",
    "    'Dbm_1',\n",
    "    'SignalLevel_1',\n",
    "    'RNCID_2',\n",
    "    'CellID_2',\n",
    "    'AsuLevel_2',\n",
    "    'Dbm_2',\n",
    "    'SignalLevel_2',\n",
    "    'RNCID_3',\n",
    "    'CellID_3',\n",
    "    'AsuLevel_3',\n",
    "    'Dbm_3',\n",
    "    'SignalLevel_3',\n",
    "    'RNCID_4',\n",
    "    'CellID_4',\n",
    "    'AsuLevel_4',\n",
    "    'Dbm_4',\n",
    "    'SignalLevel_4',\n",
    "    'RNCID_5',\n",
    "    'CellID_5',\n",
    "    'AsuLevel_5',\n",
    "    'Dbm_5',\n",
    "    'SignalLevel_5',\n",
    "    'RNCID_6',\n",
    "    'CellID_6',\n",
    "    'AsuLevel_6',\n",
    "    'Dbm_6',\n",
    "    'SignalLevel_6',\n",
    "    #'RSSI_6',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_rf = [\n",
    "    'RNCID_1',\n",
    "    'CellID_1',\n",
    "    'AsuLevel_1',\n",
    "    'Dbm_1',\n",
    "    'SignalLevel_1',\n",
    "    'RNCID_2',\n",
    "    'CellID_2',\n",
    "    'AsuLevel_2',\n",
    "    'Dbm_2',\n",
    "    'SignalLevel_2',\n",
    "    'RNCID_3',\n",
    "    'CellID_3',\n",
    "    'AsuLevel_3',\n",
    "    'Dbm_3',\n",
    "    'SignalLevel_3',\n",
    "    'RNCID_4',\n",
    "    'CellID_4',\n",
    "    'AsuLevel_4',\n",
    "    'Dbm_4',\n",
    "    'SignalLevel_4',\n",
    "    'RNCID_5',\n",
    "    'CellID_5',\n",
    "    'AsuLevel_5',\n",
    "    'Dbm_5',\n",
    "    'SignalLevel_5',\n",
    "    'RNCID_6',\n",
    "    'CellID_6',\n",
    "    'AsuLevel_6',\n",
    "    'Dbm_6',\n",
    "    'SignalLevel_6',\n",
    "    'Lon','Lat','Lon2','Lat2','Lon3','Lat3','Lon4','Lat4','Lon5','Lat5','Lon6','Lat6'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_2g_engpara():\n",
    "    #eng_para = pd.read_csv('DLoc/siping_2g_new_gongcan.csv', encoding='gbk')\n",
    "    eng_para = pd.read_csv('2g/siping_2g_new_gongcan.csv', encoding='gbk')\n",
    "    eng_para = eng_para[['RNCID', 'CellID', 'BSID','Lon','Lat']]\n",
    "    #eng_para = eng_para[eng_para.LAC.notnull() & eng_para[u'经度'].notnull()]\n",
    "    eng_para = eng_para.drop_duplicates()\n",
    "    #eng_para.rename(columns={u'经度': 'lon', u'纬度': 'lat'}, inplace=True)\n",
    "    return eng_para\n",
    "\n",
    "def make_rf_dataset(data, eng_para):\n",
    "    for i in range(1, 7):\n",
    "        data = data.merge(eng_para, left_on=['RNCID_%d' % i, 'CellID_%d' % i], right_on=['RNCID','CellID'], how='left', suffixes=('', '%d' % i))\n",
    "        temp=data['CellID_%d'% i].tolist()\n",
    "        new=list()\n",
    "        for item in temp:\n",
    "            if math.isnan(item):\n",
    "                new.append(0)\n",
    "            elif int(item)<=0:\n",
    "                new.append(0)\n",
    "            else:\n",
    "                new.append(item)\n",
    "        data['CellID_%d' % i]=new\n",
    "    data = data.fillna(-999.)\n",
    "    #print data.columns\n",
    "    \n",
    "    feature = data[col_name_new+['mode','MRTime','BSID','BSID2','BSID3','BSID4','BSID5','BSID6','Longitude', 'Latitude',\n",
    "                                'Lon','Lat','Lon2','Lat2','Lon3','Lat3','Lon4','Lat4','Lon5','Lat5','Lon6','Lat6']]\n",
    "   \n",
    "    \n",
    "    subset=[u'Longitude', u'Latitude', \n",
    "       u'RNCID_1', u'CellID_1',u'Dbm_1',\n",
    "       u'RNCID_2', u'CellID_2',u'Dbm_2',\n",
    "       u'RNCID_3', u'CellID_3',u'Dbm_3',\n",
    "       u'RNCID_4', u'CellID_4',u'Dbm_4',\n",
    "       u'RNCID_5', u'CellID_5',u'Dbm_5',\n",
    "       u'RNCID_6', u'CellID_6',u'Dbm_6',\n",
    "       ]\n",
    "    #feature=feature.drop_duplicates(subset=subset) \n",
    "    label = feature[['Longitude', 'Latitude']]\n",
    "    feature= feature.drop(['Longitude', 'Latitude'],axis=1)\n",
    "    \n",
    "    return feature, label\n",
    "\n",
    "#eng_para = merge_2g_engpara()\n",
    "eng_para =merge_2g_engpara()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_model_label(error):\n",
    "    conf_l=list()\n",
    "    \n",
    "    for t in error:\n",
    "        if t<=50:\n",
    "            conf_l.append(1)\n",
    "        else:\n",
    "            conf_l.append(0)\n",
    "    \n",
    "    return conf_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2g=pd.read_csv(\"2g/siping_2g_mode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、邻接基站实验\n",
    "bs_num = []\n",
    "for idx,row in data_2g.iterrows():\n",
    "    for i in range(1,8):\n",
    "        if row['RNCID_%d'%i]==0 or math.isnan(row['RNCID_%d'%i]) or row['CellID_%d'%i]==-1 or math.isnan(row['CellID_%d'%i]):\n",
    "            bs_num.append(i-1)\n",
    "            break\n",
    "        if i == 7:\n",
    "            bs_num.append(i)        \n",
    "data_2g['bs_num'] = bs_num\n",
    "retain_num = int(sys.argv[1])\n",
    "# retain_num = 1\n",
    "for idx,row in data_2g.iterrows():\n",
    "    if row['bs_num']-1 <= retain_num:\n",
    "        continue\n",
    "    for i in range(retain_num+2, row['bs_num']+1):\n",
    "        data_2g.loc[idx, 'RNCID_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'CellID_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'AsuLevel_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'SignalLevel_%d'%i] = np.nan\n",
    "        data_2g.loc[idx, 'Dbm_%d'%i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、基站密度实验\n",
    "bs = []\n",
    "for i in range(1, 8):\n",
    "    bs += data_2g[['RNCID_%d'% i, 'CellID_%d'% i]].values.tolist()\n",
    "bs = [tuple(t) for t in bs]\n",
    "temp = []\n",
    "[temp.append(i) for i in bs if not i in temp]\n",
    "bs = temp\n",
    "ratio = float(sys.argv[1])\n",
    "drop_bs = random.sample(bs, int(len(bs) * ratio))\n",
    "for idx, row in data_2g.iterrows():\n",
    "    for i in range(1, 7):\n",
    "        if (row['RNCID_%d'% i], row['CellID_%d'% i]) in drop_bs:\n",
    "            data_2g.loc[idx, 'RNCID_%d'% i] = -999\n",
    "            data_2g.loc[idx, 'CellID_%d'% i] = -999\n",
    "            data_2g.loc[idx, 'Dbm_%d'% i] = -999\n",
    "data_2g = data_2g.drop(data_2g[data_2g['RNCID_1']==-999].index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、运动模式实验\n",
    "mode = int(sys.argv[1])\n",
    "data_2g = data_2g[data_2g['mode'] == mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "train, label = make_rf_dataset(data_2g, eng_para)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "tr_feature_r, te_feature_r, tr_label_, te_label_ = train_test_split(train, label, test_size=0.4,random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3727"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_feature_r.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_pre(data):\n",
    "    data=data.iloc[:,1:]\n",
    "    label=data[['Longitude','Latitude']]\n",
    "    data=data.drop(['Longitude', 'Latitude'],axis=1)\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_tr_feature, con_te_feature, con_tr_p, con_te_p = train_test_split(te_feature_r, te_label_, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid\n",
    "#from grid import RoadGrid\n",
    "#importlib.reload(grid)\n",
    "#from grid import RoadGrid\n",
    "rg = grid.RoadGrid(np.vstack((tr_label_.values, te_label_.values)),100)\n",
    "tr_label_g = rg.transform(tr_label_.values, False)\n",
    "#rint tr_label_\n",
    "con_tr_j = rg.transform(con_tr_p.values, False)\n",
    "con_te_j = rg.transform(con_te_p.values, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "est=RandomForestClassifier( n_jobs=-1,\n",
    "    n_estimators =50,\n",
    "    max_features='sqrt').fit(tr_feature_r[col_name_rf].values, tr_label_g)\n",
    "    #est=DecisionTreeRegressor(max_depth=4).fit(tr_f.values, tr_l.values)\n",
    "pred_con_tr=est.predict(con_tr_feature[col_name_rf].values)\n",
    "pred_con_te=est.predict(con_te_feature[col_name_rf].values)\n",
    "tr_pred = np.array([rg.grid_center[idx] for idx in pred_con_tr])\n",
    "te_pred = np.array([rg.grid_center[idx] for idx in pred_con_te])\n",
    "error_tr = [distance(pt1, pt2) for pt1, pt2 in zip(tr_pred, con_tr_p.values)]\n",
    "#error_tr = sorted(error_tr)\n",
    "error_te = [distance(pt1, pt2) for pt1, pt2 in zip(te_pred, con_te_p.values)]\n",
    "#error_te = sorted(error_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tr_feature_r['Longitude'] = tr_label_.iloc[:, 0].values\n",
    "tr_feature_r['Latitude'] = tr_label_.iloc[:, 1].values\n",
    "tr_feature_r['gid'] = tr_label_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_feature_r.to_csv(\"2g/total_conf_tr_sp2g.csv\")\n",
    "#con_tr_feature.to_csv(\"2g/conf_tr_sp2g.csv\")\n",
    "#con_te_feature.to_csv(\"2g/conf_te_sp2g.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_label_tr = conf_model_label(error_tr)\n",
    "conf_label_te = conf_model_label(error_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con_tr_feature[\"conf\"]= conf_label_tr\n",
    "con_te_feature[\"conf\"]= conf_label_te\n",
    "con_tr_feature[\"error\"]= error_tr\n",
    "con_te_feature[\"error\"]= error_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con_tr_feature['Longitude'] = con_tr_p.iloc[:,0]\n",
    "con_tr_feature['Latitude'] = con_tr_p.iloc[:,1]\n",
    "con_te_feature['Longitude'] = con_te_p.iloc[:,0]\n",
    "con_te_feature['Latitude'] = con_te_p.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con_tr_feature['p_gid'] = pred_con_tr\n",
    "con_tr_feature['gid'] = con_tr_j\n",
    "con_te_feature['p_gid'] = pred_con_te\n",
    "con_te_feature['gid'] = con_te_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss_level(dbm):\n",
    "    if dbm>-50:\n",
    "        return 1\n",
    "    elif dbm >-60:\n",
    "        return 2\n",
    "    elif dbm >-70:\n",
    "        return 3\n",
    "    elif dbm>-80:\n",
    "        return 4\n",
    "    elif dbm>-90:\n",
    "        return 5\n",
    "    elif dbm>-100:\n",
    "        return 6\n",
    "    elif dbm>-110:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 7):\n",
    "    con_tr_feature['rss_level_%d' % i] = con_tr_feature['Dbm_%d' % i].map(lambda x: rss_level(x))  \n",
    "    con_te_feature['rss_level_%d' % i] = con_te_feature['Dbm_%d' % i].map(lambda x: rss_level(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_te_feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob = con_tr_feature[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "               'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6',]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_bs = con_tr_feature[['BSID','BSID2','BSID3','BSID4','BSID5','BSID6',]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_rss = con_tr_feature[['rss_level_1','rss_level_2','rss_level_3',\n",
    "                               'rss_level_4','rss_level_5','rss_level_6',]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_te = con_te_feature[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "               'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6',]].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ob_te.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(list1, list2):\n",
    "    #print list1, list2\n",
    "    union_set = len(set(list1)|set(list2))#并集长度\n",
    "    intersection_set = len(set(list1)&set(list2))#交集长度\n",
    "\n",
    "    Jaccard = float(intersection_set/union_set) #Jaccar\n",
    "    return Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_emission_pro(jd_list, match, bs_list, ss_list, can_list, total_c, conf):\n",
    "    pro_list = []\n",
    "    weight_list = []\n",
    "    if match.shape[0]>0:\n",
    "        weight_list.append(math.log(1+match.shape[0])*1)\n",
    "        match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "             & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5])]\n",
    "        if match_ss.shape[0]>0:\n",
    "            pro_list.append(float(match_ss[match_ss['conf']==conf].shape[0])/float(total_c))\n",
    "        else:\n",
    "            pro_list.append(float(match[match['conf']==conf].shape[0])/float(total_c))\n",
    "        \n",
    "        \n",
    "    for can_temp, jd in zip(can_list, jd_list):\n",
    "        match_c = con_tr_feature[(con_tr_feature['BSID']==int(can_temp[0])) & (con_tr_feature['BSID2']==int(can_temp[1]))\n",
    "                      &(con_tr_feature['BSID3']==int(can_temp[2])) & (con_tr_feature['BSID4']==int(can_temp[3]))\n",
    "                      &(con_tr_feature['BSID5']==int(can_temp[4])) & (con_tr_feature['BSID6']==int(can_temp[5]))]\n",
    "        count = match_c.shape[0]\n",
    "        weight_list.append(math.log(1+count)*jd)\n",
    "        \n",
    "        match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                            & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                            & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])]\n",
    "        if match_ss_c.shape[0]>0:\n",
    "            pro_list.append(float(match_ss_c[match_ss_c['conf']==conf].shape[0])/float(total_c))\n",
    "        else:\n",
    "            pro_list.append(float(match_c[match_c['conf']==conf].shape[0])/float(total_c))\n",
    "\n",
    "\n",
    "    weight_sum = np.sum(weight_list)\n",
    "    ad_em_po = 0\n",
    "    \n",
    "    for x, y in zip(pro_list, weight_list):\n",
    "        ad_em_po += x * (y / weight_sum)\n",
    "    \n",
    "    return ad_em_po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_list = []\n",
    "i =0\n",
    "j=0\n",
    "zero_num = con_tr_feature[con_tr_feature['conf']==0].shape[0]\n",
    "for idx, row in total_ob_te.iterrows():\n",
    "    bs_list =row[['BSID','BSID2','BSID3','BSID4','BSID5','BSID6']].values\n",
    "    ss_list = row[['rss_level_1','rss_level_2','rss_level_3','rss_level_4','rss_level_5','rss_level_6',]].values\n",
    "    #print bs_list[0]\n",
    "    match = con_tr_feature[(con_tr_feature['BSID']==int(bs_list[0])) & (con_tr_feature['BSID2']==int(bs_list[1]))\n",
    "                          &(con_tr_feature['BSID3']==int(bs_list[2])) & (con_tr_feature['BSID4']==int(bs_list[3]))\n",
    "                          &(con_tr_feature['BSID5']==int(bs_list[4])) & (con_tr_feature['BSID6']==int(bs_list[5]))]\n",
    "    if match.shape[0]<5:\n",
    "        can_bs_row = []\n",
    "        can_j = []\n",
    "        jaccd_max = []\n",
    "        idx_list = []\n",
    "        for idxx, roww in total_ob_bs.iterrows():\n",
    "            can_bs_list = roww.values\n",
    "            jd = jaccard_sim(bs_list, can_bs_list)\n",
    "            jaccd_max.append(jd)\n",
    "            idx_list.append(idxx)\n",
    "            if jd>0.5:\n",
    "                can_bs_row.append(can_bs_list)\n",
    "                can_j.append(jd)\n",
    "                \n",
    "        if len(can_bs_row)==0:\n",
    "            can_idx = jaccd_max.index(np.max(jaccd_max))\n",
    "            can_temp = total_ob_bs.iloc[can_idx,:].values\n",
    "            can_bs_row.append(total_ob_bs.iloc[can_idx,:].values)\n",
    "            match_c = con_tr_feature[(con_tr_feature['BSID']==int(can_temp[0])) & (con_tr_feature['BSID2']==int(can_temp[1]))\n",
    "                          &(con_tr_feature['BSID3']==int(can_temp[2])) & (con_tr_feature['BSID4']==int(can_temp[3]))\n",
    "                          &(con_tr_feature['BSID5']==int(can_temp[4])) & (con_tr_feature['BSID6']==int(can_temp[5]))]\n",
    "            \n",
    "            match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                            & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                            & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])]\n",
    "            if match_ss_c.shape[0]>0:\n",
    "                zeros_list.append(float(match_ss_c[match_ss_c['conf']==0].shape[0])/float(zero_num))\n",
    "            else:\n",
    "                zeros_list.append(float(match_c[match_c['conf']==0].shape[0])/float(zero_num))\n",
    "        \n",
    "            j+=1 \n",
    "        else:\n",
    "            zeros_list.append(adaptive_emission_pro(can_j, match, bs_list, ss_list, can_bs_row, zero_num, 0))\n",
    "        #print i, len(can_bs_row)\n",
    "        i+=1\n",
    "    else:\n",
    "        j+=1\n",
    "        match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "             & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5])]\n",
    "        if match_ss.shape[0]>0:\n",
    "            zeros_list.append(float(match_ss[match_ss['conf']==0].shape[0])/float(zero_num))\n",
    "        else:\n",
    "            zeros_list.append(float(match[match['conf']==0].shape[0])/float(zero_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_te['conf_ad_em_pro_0'] = zeros_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_list = []\n",
    "one_num = con_tr_feature[con_tr_feature['conf']==1].shape[0]\n",
    "for idx, row in total_ob_te.iterrows():\n",
    "    bs_list =row[['BSID','BSID2','BSID3','BSID4','BSID5','BSID6']].values\n",
    "    ss_list = row[['rss_level_1','rss_level_2','rss_level_3','rss_level_4','rss_level_5','rss_level_6',]].values\n",
    "    #print bs_list[0]\n",
    "    match = con_tr_feature[(con_tr_feature['BSID']==int(bs_list[0])) & (con_tr_feature['BSID2']==int(bs_list[1]))\n",
    "                          &(con_tr_feature['BSID3']==int(bs_list[2])) & (con_tr_feature['BSID4']==int(bs_list[3]))\n",
    "                          &(con_tr_feature['BSID5']==int(bs_list[4])) & (con_tr_feature['BSID6']==int(bs_list[5]))]\n",
    "    if match.shape[0]<5:\n",
    "        can_bs_row = []\n",
    "        can_j = []\n",
    "        jaccd_max = []\n",
    "        idx_list = []\n",
    "        for idxx, roww in total_ob_bs.iterrows():\n",
    "            can_bs_list = roww.values\n",
    "            jd = jaccard_sim(bs_list, can_bs_list)\n",
    "            jaccd_max.append(jd)\n",
    "            idx_list.append(idxx)\n",
    "            if jd>0.5:\n",
    "                can_bs_row.append(can_bs_list)\n",
    "                can_j.append(jd)\n",
    "                \n",
    "        if len(can_bs_row)==0:\n",
    "            can_idx = jaccd_max.index(np.max(jaccd_max))\n",
    "            can_temp = total_ob_bs.iloc[can_idx,:].values\n",
    "            can_bs_row.append(total_ob_bs.iloc[can_idx,:].values)\n",
    "            match_c = con_tr_feature[(con_tr_feature['BSID']==int(can_temp[0])) & (con_tr_feature['BSID2']==int(can_temp[1]))\n",
    "                          &(con_tr_feature['BSID3']==int(can_temp[2])) & (con_tr_feature['BSID4']==int(can_temp[3]))\n",
    "                          &(con_tr_feature['BSID5']==int(can_temp[4])) & (con_tr_feature['BSID6']==int(can_temp[5]))]\n",
    "            \n",
    "            match_ss_c= match_c[(match_c['rss_level_1']== ss_list[0]) & (match_c['rss_level_2']== ss_list[1]) \n",
    "                            & (match_c['rss_level_3']==ss_list[2])& (match_c['rss_level_4']==ss_list[3]) \n",
    "                            & (match_c['rss_level_5']==ss_list[4]) & (match_c['rss_level_6']==ss_list[5])]\n",
    "            if match_ss_c.shape[0]>0:\n",
    "                one_list.append(float(match_ss_c[match_ss_c['conf']==1].shape[0])/float(one_num))\n",
    "            else:\n",
    "                one_list.append(float(match_c[match_c['conf']==1].shape[0])/float(one_num))\n",
    "        \n",
    "        else:\n",
    "            one_list.append(adaptive_emission_pro(can_j, match, bs_list, ss_list, can_bs_row, one_num, 1))\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        match_ss= match[(match['rss_level_1']== int(ss_list[0])) & (match['rss_level_2']==int(ss_list[1])) & (match['rss_level_3']==ss_list[2])\n",
    "             & (match['rss_level_4']==ss_list[3]) & (match['rss_level_5']==ss_list[4]) & (match['rss_level_6']==ss_list[5])]\n",
    "        if match_ss.shape[0]>0:\n",
    "            one_list.append(float(match_ss[match_ss['conf']==1].shape[0])/float(one_num))\n",
    "        else:\n",
    "            one_list.append(float(match[match['conf']==1].shape[0])/float(one_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ob_te['conf_ad_em_pro_1'] = one_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "con_te_feature['gid'] = con_te_j\n",
    "con_te_feature['p_gid'] = pred_con_te\n",
    "con_tr_feature['gid'] = con_tr_j\n",
    "con_tr_feature['p_gid'] = pred_con_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_tr = con_tr_feature.groupby(['TrajID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = []\n",
    "st_mat = np.zeros((14,2,2)) #0-0 0-1 1-0, 1-1\n",
    "for trajid, traj in trajs_tr:\n",
    "    traj = traj.sort_values(by=['MRTime'],ascending=True)\n",
    "    t_time = traj['MRTime'].values\n",
    "    conf = traj['conf'].values\n",
    "    for i in range(traj.shape[0]-1):\n",
    "        time_list.append(compute_time_interval(t_time[i], t_time[i + 1]))\n",
    "        idx = int(compute_time_interval(t_time[i], t_time[i + 1])/5)\n",
    "        if idx >12:\n",
    "            idx = 13\n",
    "        if conf[i]==0 and conf[i+1]==0:\n",
    "            st_mat[idx, 0, 0] +=1\n",
    "        if conf[i] ==0 and conf[i+1] ==1:\n",
    "            st_mat[idx, 0, 1] +=1\n",
    "        if conf[i]==1 and conf[i+1]==0:\n",
    "            st_mat[idx, 1, 0] +=1\n",
    "        if conf[i] ==1 and conf[i+1] ==1:\n",
    "            st_mat[idx, 1, 1] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_te = con_te_feature.groupby(['TrajID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_t_idx = []\n",
    "#st_mat = np.zeros((14,2,2)) #0-0 0-1 1-0, 1-1\n",
    "pred_list = []\n",
    "p_g_list = []\n",
    "t_g_list = []\n",
    "for trajid, traj in trajs_te:\n",
    "    traj = traj.sort_values(by=['MRTime'],ascending=True)\n",
    "    t_time = traj['MRTime'].values\n",
    "    conf = traj['conf'].values\n",
    "    idx_list = []\n",
    "    for i in range(traj.shape[0]-1):\n",
    "        time_list.append(compute_time_interval(t_time[i], t_time[i + 1]))\n",
    "        idx = int(compute_time_interval(t_time[i], t_time[i + 1])/5)\n",
    "        if idx >12:\n",
    "            idx = 13\n",
    "        idx_list.append(idx)\n",
    "    pred_list.append(traj[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "               'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6',]].values)\n",
    "        \n",
    "    pred_list_t_idx.append(idx_list)\n",
    "    p_g_list.append(traj['p_gid'].values)\n",
    "    t_g_list.append(traj[['Longitude','Latitude']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prob = [float(con_tr_feature[con_tr_feature['conf']==0].shape[0])/con_tr_feature.shape[0],\n",
    "             float(con_tr_feature[con_tr_feature['conf']==1].shape[0])/con_tr_feature.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_viterbi(st_mat, init_prob, emit_prob, obs_seq, t_list):\n",
    "    Nstate = 2\n",
    "    Nobs = int(emit_prob.shape[0])\n",
    "    T = len(obs_seq)\n",
    "    \n",
    "    partial_prob = np.zeros((Nstate,T))\n",
    "\n",
    "    path = np.zeros((Nstate,T))\n",
    "\n",
    "    for i in range(Nstate):\n",
    "        partial_prob[i,0] = init_prob[i] * emit_prob[obs_seq[0], i]\n",
    "        path[i,0] = i\n",
    "\n",
    "\n",
    "    for t in range(1,T,1):\n",
    "        newpath = np.zeros((Nstate,T))\n",
    "        for i in range(Nstate):\n",
    "            prob = -1.0\n",
    "            for j in range(Nstate):\n",
    "                nprob = partial_prob[j,t-1] * st_mat[t_list[t-1], j, i] * emit_prob[obs_seq[t], i]\n",
    "                if nprob > prob:\n",
    "                    prob = nprob\n",
    "                    partial_prob[i,t] = nprob\n",
    "                    newpath[i,0:t] = path[j,0:t]\n",
    "                    newpath[i,t] = i\n",
    "        path = newpath\n",
    "    \n",
    "    prob = -1.0\n",
    "    j = 0\n",
    "    for i in range(Nstate):\n",
    "        if(partial_prob[i,T-1] > prob):\n",
    "            prob = partial_prob[i,T-1]\n",
    "            j = i\n",
    "\n",
    "    return path[j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_pro = total_ob_te\n",
    "em_pro = em_pro.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_prob = em_pro[['conf_ad_em_pro_0', 'conf_ad_em_pro_1']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_g = rg.n_grid\n",
    "w = np.zeros((num_g, num_g))\n",
    "\n",
    "test = zip(pred_con_tr, con_tr_j, error_tr)\n",
    "tj_r = {}\n",
    "for ss in test:\n",
    "    #print ss\n",
    "    #w[ss[0]][num_g]=w[ss[0]][num_g]+1\n",
    "    #w[ss[0]][ss[1]]=w[ss[0]][ss[1]]+1\n",
    "    if ss[0]!=ss[1] and ss[2]>50:\n",
    "         w[ss[0]][ss[1]] +=1\n",
    "        #w[ss[0], ss[1]] += 1\n",
    "    if not tj_r.has_key(ss[0]):\n",
    "        tj_r[ss[0]]=set()\n",
    "    tj_r[ss[0]].add((ss[1]))\n",
    "\n",
    "standard_point=[]\n",
    "\n",
    "\n",
    "for item in tj_r:\n",
    "    if (len(tj_r[item])==1) and (item in tj_r[item]):\n",
    "        standard_point.append(item)\n",
    "\n",
    "for idx in range(num_g):\n",
    "    if not tj_r.has_key(idx):\n",
    "        tj_r[idx]=set()\n",
    "        tj_r[idx].add((idx))\n",
    "\n",
    "g_list=rg.gridlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_obs = {}\n",
    "for idx, row in con_tr_feature.iterrows():\n",
    "    obs = row[['BSID','rss_level_1','BSID2','rss_level_2',\n",
    "               'BSID3','rss_level_3','BSID4','rss_level_4',\n",
    "               'BSID5','rss_level_5','BSID6','rss_level_6',]].values\n",
    "    gid = int(row['gid'])\n",
    "    match = em_pro[(em_pro['BSID']==obs[0]) & (em_pro['rss_level_1']==obs[1]) & \n",
    "                      (em_pro['BSID2']==obs[2]) & (em_pro['rss_level_2']==obs[3]) & \n",
    "                      (em_pro['BSID3']==obs[4]) & (em_pro['rss_level_3']==obs[5]) & \n",
    "                      (em_pro['BSID4']==obs[6]) & (em_pro['rss_level_4']==obs[7]) & \n",
    "                      (em_pro['BSID5']==obs[8]) & (em_pro['rss_level_5']==obs[9]) & \n",
    "                      (em_pro['BSID6']==obs[10]) & (em_pro['rss_level_6']==obs[11])]\n",
    "    if match.shape[0]>0:\n",
    "        obs_idx = (gid, int(match.index[0]))\n",
    "        if not g_obs.has_key(obs_idx):\n",
    "            g_obs[obs_idx]=1\n",
    "        else:\n",
    "            g_obs[obs_idx] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(te_pred, cof_list, o_list):\n",
    "    i=0\n",
    "    te_pred_n=[]\n",
    "    while i < len(cof_list):\n",
    "        if cof_list[i]==0:\n",
    "            pred_temp = te_pred[i]\n",
    "            if pred_temp in standard_point:\n",
    "                te_pred_n.append(te_pred[i])\n",
    "            else:\n",
    "                repair_r = list(w[te_pred[i], :])\n",
    "                max_idx = repair_r.index(np.max(repair_r))\n",
    "                if g_obs.has_key((max_idx, o_list[i])):\n",
    "                    te_pred_n.append(int(max_idx))\n",
    "                    #print 'ok'\n",
    "                else:\n",
    "                    \n",
    "                    te_pred_n.append(te_pred[i])\n",
    "                \n",
    "        else:\n",
    "            te_pred_n.append(te_pred[i])\n",
    "        i+=1\n",
    "            \n",
    "    #print len(te_pred_n)\n",
    "    return te_pred_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "error_list = []\n",
    "error_new =[]\n",
    "i=0\n",
    "for row, time, raw, gt in zip (pred_list, pred_list_t_idx, p_g_list, t_g_list):\n",
    "\n",
    "    o_seq = []\n",
    "    for obs in row:\n",
    "        #print obs\n",
    "        match = em_pro[(em_pro['BSID']==obs[0]) & (em_pro['rss_level_1']==obs[1]) & \n",
    "                      (em_pro['BSID2']==obs[2]) & (em_pro['rss_level_2']==obs[3]) & \n",
    "                      (em_pro['BSID3']==obs[4]) & (em_pro['rss_level_3']==obs[5]) & \n",
    "                      (em_pro['BSID4']==obs[6]) & (em_pro['rss_level_4']==obs[7]) & \n",
    "                      (em_pro['BSID5']==obs[8]) & (em_pro['rss_level_5']==obs[9]) & \n",
    "                      (em_pro['BSID6']==obs[10]) & (em_pro['rss_level_6']==obs[11])]\n",
    "        o_seq.append(match.index[0])\n",
    "    pred = raw\n",
    "    true = gt\n",
    "    cof_list = hmm_viterbi(st_mat, init_prob, emit_prob, o_seq, time)\n",
    "    r_pred = repair(pred, cof_list, o_seq)\n",
    "    \n",
    "   \n",
    "    te_predp = np.array([rg.grid_center[idx] for idx in r_pred])\n",
    "   \n",
    "   \n",
    "    error_tep = [distance(pt1, pt2) for pt1, pt2 in zip(te_predp, true)]\n",
    "   \n",
    "   \n",
    "    for t in error_tep:\n",
    "        #print t\n",
    "        error_new.append(t)\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "err= sorted(error_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1 32.1152763819 52.2\n"
     ]
    }
   ],
   "source": [
    "print np.median(err), np.mean(err), err[int(len(err) * 0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
